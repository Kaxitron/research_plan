# Lesson 42: Backpropagation Through the Full Network

[â† Forward Pass](lesson-41-forward-pass.md) | [Back to TOC](../README.md) | [Next: Attention â†’](lesson-43-attention.md)

---

## ğŸ¯ Core Learning

- Applying the chain rule through every layer
- Gradient flow: how error propagates backward
- Vanishing and exploding gradients: why deep networks were historically hard
- Residual connections: the trick that solved vanishing gradients
- Batch normalization: keeping activations well-behaved
- Weight initialization: random but scaled correctly

## ğŸ“º Watch â€” Primary

1. **Welch Labs â€” "The F=ma of Artificial Intelligence [Backpropagation, How Models Learn Part 2]"**
   - https://www.youtube.com/@WelchLabs (search "How Models Learn Part 2")
   - *30 minutes. Welch frames backprop as the fundamental law of deep learning â€” the way F=ma is the fundamental law of classical mechanics. The geometric treatment of gradients flowing backward through the folded-space picture from Part 1 is exceptional.*

2. **Karpathy â€” "Becoming a Backprop Ninja" (Lecture 5)**
   - https://www.youtube.com/watch?v=q8SA3rM6ckI
   - *You manually backpropagate through an entire MLP. Painful and transformative.*

3. **Karpathy â€” "Activations & Gradients, BatchNorm" (Lecture 4)**
   - https://www.youtube.com/watch?v=P6sfmUTpUmc

## ğŸ“º Watch â€” Secondary

4. **3Blue1Brown â€” "Backpropagation, intuitively" | Deep Learning Ch. 3**
   - https://www.youtube.com/watch?v=Ilg3gGewQ5U
5. **3Blue1Brown â€” "Backpropagation calculus" | Deep Learning Ch. 4**
   - https://www.youtube.com/watch?v=tIeHLnjs5U8

## ğŸ“– Read

- **Stanford CS231n â€” "Backpropagation" notes**
  - https://cs231n.github.io/optimization-2/

## ğŸ”¨ Do

- **The Backprop Ninja exercise:** manually compute gradients through cross-entropy, linear layers, tanh, batchnorm â€” no autograd

## ğŸ”— ML Connection

Residual connections create "information highways" central to how transformers work. The residual stream IS the backbone of transformer architecture. Understanding gradient flow explains *why* transformers have the structure they do.
