# Lesson 41: The Forward Pass as Matrix Multiplications

[â† Single Neuron](lesson-40-single-neuron.md) | [Back to TOC](../README.md) | [Next: Backpropagation â†’](lesson-42-backprop.md)

---

## ğŸ¯ Core Learning

- A layer of neurons = single matrix multiplication + bias + activation
- The forward pass: input â†’ layer 1 â†’ layer 2 â†’ ... â†’ output
- This is just composition of functions! fâ‚ƒ(fâ‚‚(fâ‚(x)))
- Batch processing: multiple inputs simultaneously (wider matrices)
- Every matrix IS a linear transformation â€” Lessons 2-9 apply directly
- Softmax: turning raw scores into probabilities
- **The geometric view:** each ReLU "folds" space; layers compose these folds into rich, piecewise-linear decision boundaries

## ğŸ“º Watch â€” Primary

1. **Welch Labs â€” "The Misconception that Almost Stopped AI [How Models Learn Part 1]"**
   - https://www.youtube.com/watch?v=PFDMhdDR_1M
   - *ğŸ“– Book: Welch Labs, "The Illustrated Guide to AI," Ch. 2: Gradient Descent + Ch. 4: Deep Learning*
   - *22 minutes. The Belgium/Netherlands map example is PERFECT for your geometric intuition style. Each neuron folds a plane; stacking layers composes folds. You'll see WHY deep networks work. This is the single best visual for understanding what a forward pass actually does to data.*

2. **Karpathy â€” "Building makemore" (Lecture 2)**
   - https://www.youtube.com/watch?v=PaCmpygFfXo
3. **Karpathy â€” "Building makemore Part 2: MLP" (Lecture 3)**
   - https://www.youtube.com/watch?v=TCH_1BHY58I

## ğŸ“º Watch â€” Secondary

4. **Welch Labs â€” "Why Deep Learning Works Unreasonably Well" (Ch. 4: Deep Learning)**
   - https://www.youtube.com/watch?v=qx7hirqgfuU
   - *Shows why depth matters more than width, with the Belgium/Netherlands example extended to multi-layer networks.*

4. **Nelson Elhage â€” "Transformers for Software Engineers"** (blog/talk)
   - https://blog.nelhage.com/post/transformers-for-software-engineers/

## ğŸ“º Watch â€” Optional

5. **Andrej Karpathy â€” "Building a WaveNet"**
   - https://www.youtube.com/watch?v=t3YJ5hKiMQ0
   - *Extends the makemore series with a more advanced architecture. Optional but deepens understanding of how network depth processes information.*

## ğŸ“– Read

- **Nelson Elhage â€” "Transformers for Software Engineers"**
  - https://blog.nelhage.com/post/transformers-for-software-engineers/

## ğŸ”¨ Do

- Implement a 2-layer MLP from scratch in NumPy (no PyTorch)
- Train on MNIST digit classification
- Visualize weight matrices as images â€” what patterns do they learn?
- **Key geometric exercise:** For a 2D classification problem, visualize the decision boundary at each layer. Watch the folds happen.

## ğŸ”— ML & Alignment Connection

The **residual stream** â€” the running sum that each layer reads from and writes to â€” is the central object of mechanistic interpretability. When interpretability researchers say "the model moves information through the residual stream," they mean each matrix multiplication's output gets added back to this running total. Understanding the forward pass as composed matrix multiplications is THE prerequisite for reading Anthropic's papers.

The Welch Labs "folding" framing also explains why **depth matters more than width**: adding a layer adds another fold; adding width just makes each fold more complex. Alignment researchers trace how safety-relevant information (e.g., "this request is harmful") flows through the residual stream, which heads amplify or suppress it, and where the model "decides" to refuse or comply. The forward pass isn't just computation; it's a decision-making process you can reverse-engineer.
