# Lesson 36: Bayesian Computation ‚Äî Making the Intractable Tractable

[‚Üê Bayesian Foundations](lesson-35-bayesian-foundations.md) | [Back to TOC](../README.md) | [Next: Bayesian Model Comparison ‚Üí](lesson-37-bayesian-model-comparison.md)

---

> **Why this lesson exists:** The Bayesian posterior P(Œ∏|D) ‚àù P(D|Œ∏)P(Œ∏) is beautiful in theory but almost never computable in closed form. For neural networks with millions of parameters, the posterior is a distribution over a million-dimensional space ‚Äî you can't even write it down, let alone integrate over it. This lesson covers the computational methods that make Bayesian inference practical: MCMC (sampling from the posterior), variational inference (approximating it with a simpler distribution), and the modern hybrids that power Bayesian deep learning.

## üéØ Core Concepts

### The Computational Problem

- **The posterior** P(Œ∏|D) = P(D|Œ∏)P(Œ∏)/P(D) requires computing P(D) = ‚à´ P(D|Œ∏)P(Œ∏) dŒ∏ ‚Äî an integral over the entire parameter space. For anything beyond simple conjugate models, this integral is intractable.
- **Two strategies:** (1) **Sample** from the posterior without computing the integral (MCMC). (2) **Approximate** the posterior with a simpler distribution (variational inference).
- **The tradeoff:** MCMC gives exact samples (in the limit) but is slow. VI gives fast approximations but may miss important structure. Modern methods try to get the best of both.

### Markov Chain Monte Carlo (MCMC)

- **The key insight:** you can construct a Markov chain whose stationary distribution IS the posterior. Run the chain long enough, and the samples approximate the posterior ‚Äî without ever computing the normalizing constant.
- **Metropolis-Hastings algorithm:** propose a new parameter Œ∏' from a proposal distribution. Accept with probability min(1, P(Œ∏'|D)/P(Œ∏|D)) ‚Äî note the normalizing constants cancel! You only need the unnormalized posterior P(D|Œ∏)P(Œ∏).
- **Gibbs sampling:** a special case where you update one parameter at a time, sampling from the conditional distribution of each parameter given all others. When these conditionals are conjugate, each step is analytical.
- **Practical issues:** burn-in (initial samples before convergence), thinning (reducing correlation between samples), diagnostics (R-hat, effective sample size), mixing (chains should explore the full posterior, not get stuck in one mode).

### Hamiltonian Monte Carlo (HMC) ‚Äî Physics-Informed Sampling

- **HMC treats the posterior as a potential energy landscape** and simulates a physical system rolling on it. The "ball" explores the landscape efficiently by using gradient information.
- **Leapfrog integration:** HMC uses the gradient ‚àálog P(Œ∏|D) to simulate Hamiltonian dynamics. The trajectory explores the posterior much more efficiently than random walk proposals ‚Äî it can travel long distances in parameter space without losing acceptance probability.
- **This IS gradient-based exploration** of the posterior. You already know how to compute gradients (backprop). HMC uses the same gradients but for sampling instead of optimization.
- **NUTS (No-U-Turn Sampler):** automatically tunes the trajectory length, eliminating a key hyperparameter. This is what Stan and PyMC use under the hood.

### Variational Inference (VI) ‚Äî Optimization Instead of Sampling

- **The idea:** choose a family of simple distributions Q = {q_œÜ(Œ∏)} (e.g., Gaussians parameterized by mean and variance). Find the member that's closest to the true posterior: œÜ* = argmin KL(q_œÜ || P(Œ∏|D)).
- **The ELBO:** since KL divergence involves the intractable P(D), we maximize the **Evidence Lower BOund**: ELBO(œÜ) = E_{q_œÜ}[log P(D|Œ∏)] - KL(q_œÜ || P(Œ∏)). This is tractable: the first term is the expected log-likelihood (estimated by sampling from q), and the second is the KL from the prior (often analytical for Gaussians).
- **ELBO = log P(D) - KL(q || posterior).** Maximizing ELBO simultaneously (a) makes q close to the posterior and (b) provides a lower bound on the evidence. The gap is the KL divergence from q to the true posterior.
- **Mean-field approximation:** assume q factors as a product of independent distributions for each parameter. This is crude ‚Äî it ignores all correlations ‚Äî but fast and often surprisingly effective.

### The VAE Connection ‚Äî Neural Networks Meet Bayesian Inference

- **A Variational Autoencoder** is VI applied to a generative model. The encoder network outputs the variational parameters œÜ(x); the decoder is the likelihood P(x|z); the latent z plays the role of Œ∏.
- **The VAE loss IS the negative ELBO:** reconstruction loss (expected log-likelihood) + KL penalty (KL from approximate posterior to prior). You've been doing variational inference every time you train a VAE.
- **The reparameterization trick** from Lesson 20 makes the ELBO differentiable with respect to the encoder parameters, enabling gradient-based optimization.

### Laplace Approximation ‚Äî The Quick-and-Dirty Bayesian

- **Find the MAP estimate** Œ∏_MAP (the posterior mode). Approximate the posterior as a Gaussian centered at Œ∏_MAP with covariance equal to the inverse Hessian: P(Œ∏|D) ‚âà N(Œ∏_MAP, H‚Åª¬π). This is just the second-order Taylor expansion of the log-posterior.
- **Cheap but limited:** requires only the Hessian at the MAP (which you might already have from optimization). But it can't capture multimodality, skewness, or heavy tails.
- **For neural networks:** the Hessian is huge (d√ód for d parameters), but low-rank approximations and diagonal approximations make this feasible. This connects to the Kronecker-factored approximate curvature (K-FAC) you'll see in neural network optimization.

## üì∫ Watch ‚Äî Primary

1. **StatQuest ‚Äî "Markov Chain Monte Carlo" and "Metropolis-Hastings"**
   - *Clear step-by-step walkthrough of the algorithm.*
2. **Mutual Information ‚Äî "Variational Autoencoders"**
   - *Connects VAE training to variational inference in an accessible way.*

## üì∫ Watch ‚Äî Secondary

3. **Michael Betancourt ‚Äî "A Conceptual Introduction to Hamiltonian Monte Carlo"**
   - YouTube lecture or his excellent paper of the same name. Deep physical intuition.

## üìñ Read ‚Äî Primary

- **"Bayesian Data Analysis" by Gelman et al.** ‚Äî Chapters 10‚Äì12 (MCMC methods)
- **"Pattern Recognition and Machine Learning" by Bishop** ‚Äî Chapter 10 (Variational inference)
- **"A Conceptual Introduction to Hamiltonian Monte Carlo" by Betancourt**
  - https://arxiv.org/abs/1701.02434

## üî® Do

- **Metropolis-Hastings from scratch:** sample from a bimodal distribution P(x) ‚àù exp(-x¬≤) + exp(-(x-5)¬≤). Watch the chain jump between modes. Tune the proposal variance ‚Äî too small means slow mixing, too large means low acceptance.
- **HMC from scratch:** implement leapfrog integration for the same bimodal distribution. Compare efficiency (effective samples per gradient evaluation) with random walk MH.
- **Variational inference on 2D Gaussian:** fit a diagonal Gaussian q(Œ∏) to a correlated 2D posterior. See the mean-field approximation miss the correlation. Then fit a full-covariance Gaussian and see it capture it.
- **Bayesian neural network:** train a small BNN on a 1D regression task using either MCMC (for a tiny network) or VI (mean-field Bayes-by-Backprop). Plot the predictive uncertainty ‚Äî it should widen where you have no data.

## üîó ML Connection

- **VAEs** = neural variational inference. **Diffusion models** = score matching (closely related to VI). Understanding this lesson means understanding the probabilistic foundations of the two most important generative model families.
- **Bayesian optimization** (for hyperparameter tuning) uses Gaussian process posteriors ‚Äî these are computed using the exact formulas from this lesson.
- **Uncertainty quantification** in safety-critical applications (medical AI, autonomous driving) requires Bayesian or Bayesian-approximate methods from this lesson.

## üß† Alignment Connection

- **Honest uncertainty** requires the full posterior, not just a point estimate. An AI system that can report "I'm 70% confident" rather than just "my best guess is..." is more transparent and controllable.
- **Bayesian model averaging** (integrating over the posterior rather than using a point estimate) is more robust to model misspecification ‚Äî it averages over models rather than betting on one. This is a safety property.
