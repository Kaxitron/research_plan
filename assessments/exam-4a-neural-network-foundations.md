# Exam 4A: Neural Network Foundations

**The Path to AI Alignment ‚Äî Lessons 40‚Äì46 Comprehensive Assessment**

---

| | |
|---|---|
| **Time Allowed** | 60 minutes |
| **Total Points** | 100 |
| **Materials** | Pencil, paper, calculator (no CAS). No code. |
| **Format** | 10 questions: trace forward/backward passes, attention, transformers, RL, LLM pipeline |

> **Advice:** This exam is heavily computational. Show matrix dimensions at every step. The integrative thread: **can you trace the complete path from raw input through a transformer to aligned output, identifying the math from previous phases at each stage?**

---

## Question 1 (10 pts) ‚Äî The Single Neuron

**(a)** A single neuron computes y = œÉ(w·µÄx + b) where œÉ is the sigmoid. With w = [2, ‚àí1], b = 0.5, and x = [1, 3], compute the output y.

**(b)** A single neuron with sigmoid activation draws a decision boundary in input space. For the weights above, write the equation of this boundary. What shape is it?

**(c)** Why did ReLU replace sigmoid as the default activation? Answer in terms of the vanishing gradient problem, referencing the maximum value of œÉ'(x).

**(d)** A ReLU neuron with the same weights computes y = max(0, w·µÄx + b). Compute the output for x = [1, 3]. Compare with the sigmoid output.

---

## Question 2 (12 pts) ‚Äî Forward Pass as Matrix Multiplication

A 2-layer network has:
- Input x = [1, 0, ‚àí1]·µÄ (3√ó1)
- Layer 1: W‚ÇÅ = [[1, 0, 1], [0, 1, ‚àí1]] (2√ó3), b‚ÇÅ = [0, 0]·µÄ
- Activation: ReLU
- Layer 2: W‚ÇÇ = [[1, ‚àí1]] (1√ó2), b‚ÇÇ = [0.5]

**(a)** Compute z‚ÇÅ = W‚ÇÅx + b‚ÇÅ. State the dimensions at each step.

**(b)** Compute a‚ÇÅ = ReLU(z‚ÇÅ).

**(c)** Compute z‚ÇÇ = W‚ÇÇa‚ÇÅ + b‚ÇÇ (the output).

**(d)** If we process a batch of 4 inputs simultaneously, what are the dimensions of the input matrix X, each weight matrix multiplication, and the output? Why is batch processing efficient?

---

## Question 3 (12 pts) ‚Äî Backpropagation by Hand

Using the network from Question 2 with the output z‚ÇÇ you computed, suppose the target is y = 1 and the loss is L = (z‚ÇÇ ‚àí y)¬≤.

**(a)** Compute L.

**(b)** Compute ‚àÇL/‚àÇz‚ÇÇ.

**(c)** Compute ‚àÇL/‚àÇa‚ÇÅ = W‚ÇÇ·µÄ ¬∑ (‚àÇL/‚àÇz‚ÇÇ). State dimensions.

**(d)** Compute ‚àÇL/‚àÇz‚ÇÅ by applying the ReLU derivative element-wise to ‚àÇL/‚àÇa‚ÇÅ.

**(e)** Compute ‚àÇL/‚àÇW‚ÇÅ = (‚àÇL/‚àÇz‚ÇÅ) ¬∑ x·µÄ. Verify the dimensions match W‚ÇÅ.

**(f)** In one sentence: why do vanishing gradients occur in deep networks with sigmoid activations? How do residual connections fix this?

---

## Question 4 (10 pts) ‚Äî Attention Mechanism

Consider 3 tokens with embedding dimension d = 2. The input matrix is X = [[1,0], [0,1], [1,1]] (3√ó2).

Projection matrices: W_Q = W_K = W_V = I (identity, for simplicity).

**(a)** Compute Q = XW_Q, K = XW_K, V = XW_V.

**(b)** Compute the attention score matrix S = QK·µÄ (3√ó3). What does entry S_{ij} represent?

**(c)** Apply scaled attention: S/‚àöd_k where d_k = 2. Then apply softmax to each ROW of the result. *(You may leave softmax values as approximate decimals.)*

**(d)** Compute the attention output: A = softmax(S/‚àöd_k) ¬∑ V. What has the attention mechanism done to each token's representation?

---

## Question 5 (10 pts) ‚Äî Transformer Architecture

**(a)** List the components inside a single transformer block in order. For each, state its purpose in one sentence.

**(b)** A transformer processes tokens of dimension d_model = 512 with 8 attention heads. What is the dimension of each head's Q, K, V? Why do we use multiple heads instead of one big attention?

**(c)** Positional encodings add position information to token embeddings. Why can't a transformer know word order without them? *(Hint: what is attention's relationship to the order of inputs?)*

**(d)** The residual stream hypothesis says the residual connections create a shared "communication bus" that all components read from and write to. Explain how this view simplifies understanding what transformers compute.

---

## Question 6 (10 pts) ‚Äî Reinforcement Learning Foundations

**(a)** Define: state, action, reward, policy œÄ(a|s), value function V^œÄ(s), and discount factor Œ≥. Each in one sentence.

**(b)** The policy gradient theorem states: ‚àáJ(Œ∏) = E[‚àálog œÄ(a|s; Œ∏) ¬∑ Q(s,a)]. Interpret this: what happens to the probability of an action that leads to high reward? Low reward?

**(c)** PPO (Proximal Policy Optimization) clips the policy update to stay within a "trust region." Why is this necessary? What goes wrong with unconstrained policy gradient updates?

**(d)** Give an example of reward hacking (specification gaming). Why is this the RL version of Goodhart's Law?

---

## Question 7 (10 pts) ‚Äî The LLM Training Pipeline

Trace the complete pipeline, identifying the mathematical concept from earlier phases at each stage:

**(a)** Pre-training: next-token prediction on internet text. What loss function is used? What earlier concept is this equivalent to? (Phase 3)

**(b)** Supervised Fine-Tuning (SFT): training on human-written examples. How does the loss function differ from pre-training?

**(c)** Reward Model Training: humans compare pairs of outputs. What is the reward model learning to approximate?

**(d)** RLHF with PPO: optimizing the language model against the reward model with a KL penalty. Write the RLHF objective function. Identify the role of each term.

**(e)** Name the Phase 2 concept (constrained optimization) that the KL penalty implements.

---

## Question 8 (8 pts) ‚Äî Matrix Dimensions Through a Transformer

A GPT-style model processes a sequence of 128 tokens with vocabulary size 50,000, d_model = 768, 12 attention heads, and MLP hidden size 3072.

State the dimensions of:

**(a)** The token embedding matrix.

**(b)** Q, K, V matrices for a single attention head.

**(c)** The output projection matrix that recombines heads.

**(d)** The MLP weight matrices (up-projection and down-projection).

---

## Question 9 (8 pts) ‚Äî Credit Assignment and the Reward Problem

**(a)** In RLHF, reward is given for an entire generated response, not per token. Why does this create a credit assignment problem?

**(b)** DPO (Direct Preference Optimization) skips the reward model entirely. The key insight is that the optimal RLHF policy can be expressed in closed form as a function of the preference data and the reference model. What advantage does this give over the standard RLHF pipeline?

**(c)** Both RLHF and DPO optimize for human preferences. Why might these preferences not capture "what we actually want"? Name two failure modes.

---

## Question 10 (10 pts) ‚Äî Synthesis: Math Across Phases

For each component of the LLM pipeline, identify the specific mathematical concept from the specified phase:

| Pipeline Stage | Phase 1 (Linear Algebra) | Phase 2 (Calculus) | Phase 3 (Probability) |
|---|---|---|---|
| Token embedding | ___ | ___ | ___ |
| Attention computation | ___ | ___ | ___ |
| Backpropagation | ___ | ___ | ___ |
| RLHF | ___ | ___ | ___ |

Fill in each cell with a specific concept (e.g., "dot products," "chain rule," "KL divergence"). Then pick any one cell and explain the connection in 2‚Äì3 sentences.

---

## üîß Optional Mini Project (~45 minutes): Transformer Attention from Scratch

**Build a single-layer attention mechanism in pure NumPy and visualize what it learns.**

1. Implement scaled dot-product attention from scratch: Attention(Q,K,V) = softmax(QK·µÄ/‚àöd)V
2. Create a toy sequence task: given a sequence like [3, 7, 2, 7, ?], the model must copy the token that appeared twice (the "induction" pattern)
3. Initialize random Q, K, V weight matrices. Implement the forward pass and compute the loss (cross-entropy)
4. Implement backpropagation through the attention mechanism by hand (compute all gradients analytically)
5. Train with gradient descent. Plot the attention pattern matrix at initialization vs. after training ‚Äî you should see the model learn to attend to the repeated token
6. Visualize the Q and K vectors in 2D (if d=2): show how training aligns the query of the "?" position with the key of the repeated token

**Stretch:** Add a second attention head. Show that the two heads learn different attention patterns (one might attend to position, another to token identity).

**Tools:** NumPy, Matplotlib. No PyTorch.
