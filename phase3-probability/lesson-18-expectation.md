# Lesson 18: Expectation, Variance, and Covariance

[â† Probability](lesson-17-probability.md) | [Back to TOC](../README.md) | [Next: MLE â†’](lesson-19-mle.md)

---

## ğŸ¯ Core Learning

- Expected value: the "center of mass" of a distribution
- Variance: how spread out the distribution is
- Covariance: how two variables move together
- Covariance matrices: connecting back to linear algebra (it's a matrix! with eigenvalues!)
- PCA as eigendecomposition of the covariance matrix

## ğŸ“º Watch

- **StatQuest â€” "Covariance, Clearly Explained" and "PCA, Step by Step"**
  - https://www.youtube.com/c/joshstarmer

## ğŸ“– Read

- **MML Book, Chapter 6.4â€“6.5** â€” covariance and correlation

## ğŸ”¨ Do

- Generate correlated 2D data, compute covariance matrix, find eigenvectors
- Show that PCA directions = eigenvectors of the covariance matrix
- This connects SVD â†’ covariance â†’ PCA into one unified picture

## ğŸ”— ML Connection

Batch normalization controls mean and variance of activations. The covariance structure of internal representations is crucial for understanding what networks learn. PCA on activations is a basic interpretability tool.
