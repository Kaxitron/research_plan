# Lesson 19: Maximum Likelihood Estimation

[â† Expectation](lesson-18-expectation.md) | [Back to TOC](../README.md) | [Next: Information Theory â†’](lesson-20-information-theory.md)

---

## ğŸ¯ Core Learning

- The central question: given data, what parameters best explain it?
- Likelihood function: probability of the data given the parameters
- MLE: find parameters that maximize likelihood
- Log-likelihood: turns products into sums
- MLE for Gaussians: the mean and variance formulas you know ARE MLE solutions
- **Connection to neural network training: minimizing cross-entropy loss = maximizing likelihood**

## ğŸ“– Read

- **MML Book, Chapter 8.3** â€” maximum likelihood estimation

## ğŸ”¨ Do

- Implement MLE for a Gaussian: given data points, find best Î¼ and Ïƒ
- Show that minimizing mean squared error = maximizing likelihood under Gaussian noise

## ğŸ”— ML Connection

When we train a language model to predict the next token, we are literally doing maximum likelihood estimation. The loss function (cross-entropy) IS the negative log-likelihood. This connection means you understand the fundamental training objective of every modern LLM.
