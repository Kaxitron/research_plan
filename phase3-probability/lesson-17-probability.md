# Lesson 17: Probability Distributions and Bayes' Theorem

[â† Loss Landscapes](../phase2-calculus/lesson-16-loss-landscapes.md) | [Back to TOC](../README.md) | [Next: Expectation â†’](lesson-18-expectation.md)

---

## ğŸ¯ Core Learning

- Probability as measuring uncertainty (frequentist vs. Bayesian views)
- Discrete distributions: Bernoulli, categorical (what a softmax output IS)
- Continuous distributions: Gaussian/normal (appears everywhere)
- Joint, marginal, and conditional probability
- Bayes' theorem: updating beliefs with evidence
- Prior â†’ Evidence â†’ Posterior: the fundamental pattern of learning

## ğŸ“º Watch

- **3Blue1Brown â€” "Bayes theorem, the geometry of changing beliefs"**
  - https://www.youtube.com/watch?v=HZGCoVF3YvM
- **3Blue1Brown â€” "The quick proof of Bayes' theorem"**
  - https://www.youtube.com/watch?v=U_85TaXbeIo

## ğŸ“– Read

- **MML Book, Chapter 6** â€” probability and distributions
- **"Seeing Theory"** â€” https://seeing-theory.brown.edu/ â€” beautiful interactive probability visualizations

## ğŸ”¨ Do

- Implement Bayes' theorem for a spam filter: given word frequencies, compute P(spam | words)
- Visualize 1D and 2D Gaussians (the 2D Gaussian is a contour map â€” connects to gradient work)

## ğŸ”— ML Connection

A language model's softmax output IS a categorical probability distribution over ~50,000 tokens. Bayesian reasoning is fundamental to model uncertainty and calibration â€” key alignment questions like "does the model know what it doesn't know?"
