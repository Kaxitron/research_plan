# Your First Research Project

[← Open Problems](../phase6-alignment/lesson-67-open-problems.md) | [Back to TOC](../README.md)

---

## Recommended Path (from Neel Nanda's guide)

1. **Pick a concrete open problem** from Nanda's list
   - https://www.neelnanda.io/mechanistic-interpretability/favourite-problems
   - fairly out of date, see his alignmentforum post
2. **Start small:** 1-5 day mini-projects with fast feedback loops
3. **Aim for a 1-2 week research sprint** on something that catches your interest
4. **Write up your findings** — even negative results are valuable

## Key Tools

- **TransformerLens** — mechanistic interpretability experiments
- **ARENA 3.0** — structured AI safety curriculum
  - https://github.com/callummcdougall/ARENA_3.0
- **MATS (ML Alignment Theory Scholars)** — mentorship program
- **Neel Nanda's YouTube** — research walkthroughs

## Research Communities

- **Alignment Forum** — https://www.alignmentforum.org/
- **LessWrong** — https://www.lesswrong.com/
- **EleutherAI Discord** — open-source AI research community
- **MATS program** — mentorship with alignment researchers
- **AI Safety Camp** — collaborative research sprints
