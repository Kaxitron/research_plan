# Lesson 56: Group Actions, Representations, and Neural Network Symmetry

[â† Rings & Fields](lesson-55-rings-fields.md) | [Back to TOC](../README.md) | [Next: Point-Set Topology â†’](lesson-57-point-set-topology.md)

---

> **Why this lesson exists:** A group acting on a set is the precise mathematical formulation of "symmetry of a structure." When S_n acts on weight space by permuting neurons, it creates orbits (sets of equivalent weights), stabilizers (symmetries fixing a point), and singularities (where stabilizers are unusually large). The orbit-stabilizer theorem tells you exactly how many equivalent weight configurations exist. Representations translate group actions into matrices, connecting symmetry to the linear algebra you already know. This is the direct bridge from abstract algebra to the singularities in neural network loss landscapes.

## ğŸ¯ Core Concepts

### Group Actions â€” Symmetry in Action

- **A group action** of G on a set X is a map G Ã— X â†’ X, written (g,x) â†¦ gÂ·x, satisfying: (1) eÂ·x = x, (2) (gh)Â·x = gÂ·(hÂ·x). The group "moves" elements of X while respecting the group structure.
- **The orbit** of x is Orb(x) = {gÂ·x : g âˆˆ G} â€” all positions x can be moved to. Orbits partition X.
- **The stabilizer** of x is Stab(x) = {g âˆˆ G : gÂ·x = x} â€” all symmetries that fix x. A large stabilizer means x has lots of symmetry.
- **Orbit-Stabilizer Theorem:** |Orb(x)| Ã— |Stab(x)| = |G|. More symmetry at x (bigger stabilizer) means fewer distinct positions in x's orbit.

### Neural Network Symmetries

- **Permutation symmetry:** for a fully-connected network with n hidden units, the symmetric group S_n acts on weight space by permuting units. If units i and j are swapped, the incoming and outgoing weights are swapped correspondingly, but the network function is unchanged.
- **Orbit structure:** the orbit of a weight vector w under S_n consists of all n! permutations. If two units have identical weights, the stabilizer is non-trivial â€” some permutations don't change w. This creates singularities.
- **Sign symmetry:** for ReLU networks, flipping the sign of all incoming weights to a neuron and the sign of its outgoing weight preserves the function (because ReLU(x) behaves differently, but the composition compensates). This gives an additional (â„¤â‚‚)â¿ symmetry.
- **The orbit space W/G** (weight space modulo symmetry) is the "true" parameter space â€” distinct network functions. This space has singularities at points where the stabilizer is non-trivial (i.e., where some neurons are "equivalent").

### The Orbit-Stabilizer Theorem and SLT

- **At a generic point** in weight space (all neurons distinct), the stabilizer is trivial (just the identity). The orbit has n! elements. The loss landscape locally looks like n! copies of the same shape.
- **At a singular point** (some neurons equal or zero), the stabilizer is non-trivial. The orbit is smaller â€” fewer equivalent configurations. The loss landscape develops a non-generic shape â€” a singularity.
- **The type of singularity** (measured by the RLCT) is determined by the structure of the stabilizer at that point. Larger stabilizers â†’ more severe singularities â†’ smaller RLCT â†’ lower effective complexity. This is the group-theoretic foundation of SLT.
- **Example:** in a network with 3 hidden units, if all 3 have identical weights, the stabilizer is Sâ‚ƒ (6 elements). The orbit has n!/6 = 1 element â€” the point IS fixed by the full symmetry. This is a highly singular point (the "origin" of the hidden unit space), and SLT says the RLCT here is small.

### Group Representations â€” Symmetry as Matrices

- **A representation** of G is a homomorphism Ï: G â†’ GL(V) â€” each group element becomes an invertible matrix, and the group operation becomes matrix multiplication. This lets you study abstract symmetry using linear algebra.
- **Irreducible representations** can't be decomposed into smaller ones. Every representation breaks into irreducibles â€” like eigenvectors decompose a matrix.
- **Characters** Ï‡(g) = tr(Ï(g)) are the traces of representation matrices. Characters determine the representation (up to equivalence) and are easy to compute and compare.
- **For neural networks:** the permutation action of S_n on weight space IS a representation. Decomposing it into irreducibles tells you the "independent modes" of the weight space â€” which combinations of weights are affected by which symmetries. This is used in equivariant neural network design.

### Equivariant Neural Networks

- **Equivariant layers** commute with group actions: f(gÂ·x) = gÂ·f(x). A convolutional layer is equivariant to translation: shifting the input shifts the output by the same amount.
- **Designing equivariant architectures** uses representation theory: the layer's weight matrix must commute with all representation matrices of the symmetry group. This constrains the architecture but guarantees the symmetry is respected.
- **Examples:** CNNs (translation equivariance), Spherical CNNs (rotation equivariance), Graph Neural Networks (permutation equivariance of nodes).

## ğŸ“º Watch â€” Primary

1. **Visual Group Theory â€” "Group Actions" (Nathan Carter)**
2. **Symmetry and Spectral Graph Theory lectures (YouTube)**

## ğŸ“– Read â€” Primary

- **"Visual Group Theory" by Nathan Carter** â€” Chapters 10â€“12 (actions, representations)
- **"Algebra" by Artin** â€” Chapters 5â€“6 (group actions, with excellent geometric examples)

## ğŸ”¨ Do

- Compute the orbits and stabilizers of Sâ‚ƒ acting on weight vectors of a 2-input, 3-hidden, 1-output network. Find a generic point (trivial stabilizer) and a singular point (non-trivial stabilizer).
- Implement the orbit-stabilizer theorem numerically: for each weight vector in a discretized space, compute its orbit size and stabilizer size. Verify they multiply to |G|.
- Show that a convolutional layer IS an equivariant layer: verify that conv(shift(x)) = shift(conv(x)) numerically in PyTorch.
- Representation decomposition: write the 6Ã—6 permutation matrices for Sâ‚ƒ acting on â„Â³. Find the trivial sub-representation (the all-ones direction, invariant under all permutations) and the standard representation (the orthogonal complement).
