# Video Series Index

[Back to TOC](../README.md)

## 3Blue1Brown — Essence of Linear Algebra

| # | Title | Lesson |
|---|-------|--------|
| 1 | Vectors, what even are they? | 1 |
| 2 | Linear combinations, span, and basis vectors | 2 |
| 3 | Linear transformations and matrices | 3 |
| 4 | Matrix multiplication as composition | 3-4 |
| 5 | Three-dimensional linear transformations | 3 |
| 6 | The determinant | 6 |
| 7 | Inverse matrices, column space, null space | 5 |
| 8 | Nonsquare matrices | 5 |
| 9 | Dot products and duality | 9 |
| 10 | Cross products | Optional |
| 11 | Cross products in light of transformations | Optional |
| 12 | Cramer's rule, geometrically | Optional |
| 13 | Change of basis | 10 |
| 14 | Eigenvectors and eigenvalues | 7 |
| 15 | Abstract vector spaces | 10 |

## 3Blue1Brown — Neural Networks / Deep Learning

| # | Title | Lesson |
|---|-------|--------|
| 1 | But what is a neural network? | 22 |
| 2 | Gradient descent, how neural networks learn | 13 |
| 3 | Backpropagation, intuitively | 14 |
| 4 | Backpropagation calculus | 14 |
| 5 | Transformers, the tech behind LLMs | 25 |
| 6 | Attention in transformers, step-by-step | 25 |
| 7 | How might LLMs store facts | 26 |

## Welch Labs — How Models Learn

*The geometric trilogy on why deep learning works. Watch in order.*

| # | Title | Lesson |
|---|-------|--------|
| 1 | The Misconception that Almost Stopped AI [How Models Learn Part 1] | 23 |
| 2 | The F=ma of Artificial Intelligence [Backpropagation, How Models Learn Part 2] | 24 |
| 3 | Why Deep Learning Works Unreasonably Well [How Models Learn Part 3] | 16 |

## Welch Labs — Neural Networks & AI

| # | Title | Lesson |
|---|-------|--------|
| — | ChatGPT is made from 100 million of these [The Perceptron] | 22 |
| — | The Dark Matter of AI [Mechanistic Interpretability] | 27 |
| — | How DeepSeek Rewrote the Transformer [MLA] | 26 |

## Welch Labs — Alignment & Foundations

| # | Title | Lesson |
|---|-------|--------|
| — | Can humans make AI any better? [The Bitter Lesson] | 32 |
| — | These Numbers Can Make AI Dangerous [Subliminal Learning] | 32 |
| — | What the Books Get Wrong about AI [Double Descent] | 16 |
| — | The most complex model we actually understand [Grokking] | 16 |

*All Welch Labs videos: https://www.youtube.com/@WelchLabs*

## Andrej Karpathy — Neural Networks: Zero to Hero

| # | Title | Lesson |
|---|-------|--------|
| 1 | Building micrograd (backprop from scratch) | 15 |
| 2 | Building makemore (bigrams) | 23 |
| 3 | Building makemore Part 2 (MLP) | 23 |
| 4 | Activations, gradients, BatchNorm | 24 |
| 5 | Becoming a backprop ninja | 24 |
| 6 | Building a WaveNet | Optional |
| 7 | Let's build GPT from scratch | 26 |
| 8 | Tokenization (BPE) | Optional |
| 9 | Reproducing GPT-2 | Optional |

## MIT OCW — Strang, 18.06 Key Lectures

| Lecture | Title | Lesson |
|---------|-------|--------|
| 1 | The Geometry of Linear Equations | 3 |
| 2 | Elimination with Matrices | 4 |
| 3 | Multiplication and Inverse Matrices | 3-4 |
| 5 | Transposes, Permutations, Spaces | 10 |
| 6 | Column Space and Nullspace | 5 |
| 7 | Solving Ax = 0 | 5 |
| 9 | Independence, Basis, and Dimension | 2 |
| 10 | The Four Fundamental Subspaces | 5 |
| 15 | Projections onto Subspaces | 9 |
| 16 | Projection Matrices and Least Squares | 9 |
| 17 | Orthogonal Matrices and Gram-Schmidt | 9 |
| 18 | Properties of Determinants | 6 |
| 19 | Determinant Formulas and Cofactors | 6 |
| 21 | Eigenvalues and Eigenvectors | 7 |
| 22 | Diagonalization | 7 |
| 25 | Symmetric Matrices and Positive Definiteness | 10 |
| 29 | Singular Value Decomposition | 8 |
