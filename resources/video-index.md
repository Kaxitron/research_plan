# Video Series Index

[Back to TOC](../README.md)

## 3Blue1Brown — Essence of Linear Algebra

| # | Title | Lesson |
|---|-------|--------|
| 1 | Vectors, what even are they? | 2 |
| 2 | Linear combinations, span, and basis vectors | 3 |
| 3 | Linear transformations and matrices | 4 |
| 4 | Matrix multiplication as composition | 4-5 |
| 5 | Three-dimensional linear transformations | 4 |
| 6 | The determinant | 7 |
| 7 | Inverse matrices, column space, null space | 6 |
| 8 | Nonsquare matrices | 6 |
| 9 | Dot products and duality | 10 |
| 10 | Cross products | Optional |
| 11 | Cross products in light of transformations | Optional |
| 12 | Cramer's rule, geometrically | Optional |
| 13 | Change of basis | 11 |
| 14 | Eigenvectors and eigenvalues | 8 |
| 15 | Abstract vector spaces | 11 |

## 3Blue1Brown — Neural Networks / Deep Learning

| # | Title | Lesson |
|---|-------|--------|
| 1 | But what is a neural network? | 34 |
| 2 | Gradient descent, how neural networks learn | 14 |
| 3 | Backpropagation, intuitively | 15 |
| 4 | Backpropagation calculus | 15 |
| 5 | Transformers, the tech behind LLMs | 37 |
| 6 | Attention in transformers, step-by-step | 37 |
| 7 | How might LLMs store facts | 38 |

## Welch Labs — Neural Networks Demystified & Imaginary Numbers are Real

| # | Title | Lesson |
|---|-------|--------|
| — | Neural Networks Demystified series | 18, 34–36 |
| — | Imaginary Numbers are Real | 8 |

## Andrej Karpathy — Neural Networks: Zero to Hero

| # | Title | Lesson |
|---|-------|--------|
| 1 | Building micrograd (backprop from scratch) | 16 |
| 2 | Building makemore (bigrams) | 35 |
| 3 | Building makemore Part 2 (MLP) | 35 |
| 4 | Activations, gradients, BatchNorm | 36 |
| 5 | Becoming a backprop ninja | 36 |
| 6 | Building a WaveNet | Optional |
| 7 | Let's build GPT from scratch | 38 |
| 8 | Tokenization (BPE) | Optional |
| 9 | Reproducing GPT-2 | Optional |
| — | Deep Dive into LLMs like ChatGPT | 39–40, 48 |

## MIT OCW — Strang, 18.06 Key Lectures

| Lecture | Title | Lesson |
|---------|-------|--------|
| 1 | The Geometry of Linear Equations | 4 |
| 2 | Elimination with Matrices | 5 |
| 3 | Multiplication and Inverse Matrices | 4-5 |
| 5 | Transposes, Permutations, Spaces | 11 |
| 6 | Column Space and Nullspace | 6 |
| 7 | Solving Ax = 0 | 6 |
| 9 | Independence, Basis, and Dimension | 3 |
| 10 | The Four Fundamental Subspaces | 6 |
| 15 | Projections onto Subspaces | 10 |
| 16 | Projection Matrices and Least Squares | 10 |
| 17 | Orthogonal Matrices and Gram-Schmidt | 10 |
| 18 | Properties of Determinants | 7 |
| 19 | Determinant Formulas and Cofactors | 7 |
| 21 | Eigenvalues and Eigenvectors | 8 |
| 22 | Diagonalization | 8 |
| 25 | Symmetric Matrices and Positive Definiteness | 11 |
| 29 | Singular Value Decomposition | 9 |

## StatQuest (Josh Starmer)

| Topic | Lesson |
|-------|--------|
| Covariance, PCA | 20 |
| Hypothesis Testing, P-values | 24 |
| Bayesian Inference | 23 |
| Cross-Validation, Bias-Variance | 17 |

## Neel Nanda — Mechanistic Interpretability

| Topic | Lesson |
|-------|--------|
| Paper walkthroughs | 41–42 |
| Research streams | 42, 44 |
| Getting started guide | 41 |

## Podcasts & Talks

| Source | Topic | Lesson |
|--------|-------|--------|
| Robert Miles (YouTube) | AI safety explanations | 45–49 |
| AXRP (Daniel Filan) | AI X-Risk Research | 49 |
| The Inside View | Alignment researcher interviews | 49 |
| Julia Galef (Rationally Speaking) | Decision theory, anthropics | 46–47 |
