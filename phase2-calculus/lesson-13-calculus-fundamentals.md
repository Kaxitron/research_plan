# Lesson 13: Calculus Fundamentals â€” Rebuilding Your Intuition

[â† Linear Algebra Capstone](../phase1-linear-algebra/lesson-12-capstone.md) | [Back to TOC](../README.md) | [Next: Matrix Calculus â†’](lesson-14-matrix-calculus.md)

---

> **Why this lesson exists:** You haven't touched calculus in 6 years. Lessons 14â€“27 assume fluency with derivatives, the chain rule, integration techniques, and series. This lesson rebuilds that fluency from geometric intuition â€” exactly the way we approached linear algebra â€” so that when you hit partial derivatives and gradients, they feel like natural extensions rather than cold formulas.

> **Estimated time:** 12â€“20 hours (take your time â€” this is the foundation everything else rests on)

---

## Part 1: The Big Idea â€” What IS a Derivative?

### The Geometric Picture (Start Here)

Forget the limit definition for a moment. Here's what a derivative *is*:

**A derivative measures how fast something is changing at a specific instant.**

Imagine driving a car. Your speedometer reads 60 mph. That's a derivative â€” it's telling you the *instantaneous rate of change* of your position with respect to time. You haven't actually traveled 60 miles, and you might speed up or slow down in the next second. But *right now*, at this exact instant, your position is changing at 60 miles per hour.

Geometrically, picture a curve. Pick a point on it. The derivative at that point is the **slope of the tangent line** â€” the line that just barely touches the curve at that point and matches its direction.

### From Secant Lines to Tangent Lines

Here's how to build the derivative step by step:

1. Pick two points on a curve: (x, f(x)) and (x+h, f(x+h))
2. Draw a line through them â€” that's a **secant line**
3. Its slope is: (f(x+h) - f(x)) / h â€” the "rise over run"
4. Now **shrink h toward zero**. The second point slides toward the first.
5. The secant line *rotates* and settles into the **tangent line**
6. The slope it settles on is the derivative: **f'(x)**

This is the limit definition:

$$f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$$

You don't need to compute limits from scratch in ML. But you need to *feel* what this means: the derivative is what you get when you zoom in on a curve until it looks like a straight line.

### The "Zooming In" Intuition

This is the single most important intuition for all of calculus:

**Every smooth curve, if you zoom in far enough, looks like a straight line.**

Try it: graph y = xÂ² and zoom into the point (1, 1). Keep zooming. Eventually it looks perfectly straight, with slope 2. That slope *is* the derivative at x = 1.

This is also **why gradient descent works** in neural networks. The loss surface is a complicated curved landscape. But near any point, if you zoom in enough, it looks flat â€” like a tilted plane. The gradient tells you which direction that plane tilts, and you step "downhill." Then you zoom in again at the new point, get a new local tilt, step downhill again. You're following straight-line approximations of a curved surface, one small step at a time.

### ğŸ”— ML & Alignment: Local Linearity IS Gradient Descent

When a neural network computes a gradient and takes a step, it's doing exactly this:
1. Zoom into the loss surface at the current weights
2. Approximate the surface as a flat plane (first-order Taylor approximation)
3. Step in the direction the plane tilts downhill
4. The **learning rate** controls how far you step before re-zooming

Step too far â†’ the straight-line approximation is no longer accurate â†’ you overshoot.
Step too small â†’ you waste compute on tiny improvements.
This is why learning rate matters â€” it controls how much you trust the local linear approximation.

This matters for alignment: when we train a model with RLHF, gradient descent is the mechanism that teaches the model to be helpful, harmless, and honest. Understanding *how* the gradient signal propagates â€” where it's strong, where it vanishes, where it might be gamed â€” is essential for understanding whether alignment training actually works.

---

## Part 2: The Differentiation Toolkit

You need to be able to differentiate common functions quickly and confidently. Here are the rules, each with geometric intuition.

### The Power Rule

$$\frac{d}{dx} x^n = nx^{n-1}$$

**Examples:**
- d/dx (xÂ²) = 2x â€” the slope of a parabola at x is 2x (steeper as you move away from 0)
- d/dx (xÂ³) = 3xÂ² â€” cubic curves change faster, and the rate accelerates
- d/dx (x) = 1 â€” a straight line has constant slope
- d/dx (constant) = 0 â€” flat lines have zero slope
- d/dx (xâ»Â¹) = -xâ»Â² â€” i.e., d/dx (1/x) = -1/xÂ²
- d/dx (âˆšx) = d/dx (x^(1/2)) = (1/2)x^(-1/2) = 1/(2âˆšx)

**Intuition:** The power rule says "bring the exponent down as a multiplier, then reduce the power by 1." Higher powers mean the function grows (or shrinks) faster, so the derivative is a higher-degree polynomial.

### Constant Multiple and Sum Rules

$$\frac{d}{dx}[cf(x)] = c \cdot f'(x) \qquad \frac{d}{dx}[f(x) + g(x)] = f'(x) + g'(x)$$

Derivatives are **linear operators**. If you've internalized linear algebra, this should feel familiar â€” differentiation distributes over addition and commutes with scalar multiplication. In fact, the derivative operator D is a *linear transformation* on the space of functions. This is not a coincidence; it connects directly to how we think about function spaces in ML.

### The Product Rule

$$\frac{d}{dx}[f(x) \cdot g(x)] = f'(x) \cdot g(x) + f(x) \cdot g'(x)$$

**Intuition:** Imagine a rectangle with sides f(x) and g(x). As x changes, *both* sides change. The area changes by: (change in f) Ã— g + f Ã— (change in g). The two terms capture each side's contribution while the other holds still.

**Example:** d/dx [xÂ² Â· sin(x)] = 2x Â· sin(x) + xÂ² Â· cos(x)

### The Quotient Rule

$$\frac{d}{dx}\left[\frac{f(x)}{g(x)}\right] = \frac{f'(x) \cdot g(x) - f(x) \cdot g'(x)}{[g(x)]^2}$$

**Tip:** You can usually avoid the quotient rule by rewriting f/g as f Â· gâ»Â¹ and using the product rule + chain rule instead. Many practitioners prefer this approach.

### â­ The Chain Rule â€” The Most Important Rule in All of ML

$$\frac{d}{dx} f(g(x)) = f'(g(x)) \cdot g'(x)$$

In words: **"Derivative of the outside, evaluated at the inside, times the derivative of the inside."**

**Why this is THE rule for ML:** Every neural network is a composition of functions. Layer 1 feeds into Layer 2 feeds into Layer 3 feeds into the loss. Backpropagation is literally the chain rule applied to this composition. When you hear "backprop," think "chain rule on a computation graph."

**Example â€” step by step:**

Find d/dx (xÂ² + 1)âµ

1. Identify the "outside" function: ( Â· )âµ and the "inside" function: xÂ² + 1
2. Derivative of outside: 5( Â· )â´
3. Evaluate at inside: 5(xÂ² + 1)â´
4. Multiply by derivative of inside: Ã— 2x
5. **Result:** 10x(xÂ² + 1)â´

**Chaining multiple layers:**

If you have f(g(h(x))), the chain rule gives:

$$\frac{d}{dx} f(g(h(x))) = f'(g(h(x))) \cdot g'(h(x)) \cdot h'(x)$$

Each "link" in the chain multiplies. In a 100-layer neural network, backprop multiplies through 100 chain rule factors. This is why **vanishing gradients** happen â€” if each factor is less than 1, the product shrinks exponentially.

**Example â€” nested:**

Find d/dx sin(e^(xÂ²))

1. Outermost: sin(Â·) â†’ derivative: cos(Â·) â†’ cos(e^(xÂ²))
2. Middle: e^(Â·) â†’ derivative: e^(Â·) â†’ e^(xÂ²)
3. Innermost: xÂ² â†’ derivative: 2x
4. **Multiply all three:** 2x Â· e^(xÂ²) Â· cos(e^(xÂ²))

---

## Part 3: The Cast of Characters â€” Functions You'll See Everywhere in ML

### e^x â€” The Function That Is Its Own Derivative

$$\frac{d}{dx} e^x = e^x$$

This is remarkable and unique. No other function has this property. It means e^x grows at a rate proportional to its current value â€” pure exponential growth.

**Why ML loves it:**
- The **softmax** function uses e^x to convert raw scores into probabilities
- The **natural logarithm** (inverse of e^x) appears in every loss function involving log-likelihood
- **Exponential decay** (e^(-x)) models forgetting, cooling, and learning rate schedules

**With chain rule:** d/dx e^(g(x)) = e^(g(x)) Â· g'(x)

**Example:** d/dx e^(3x) = 3e^(3x) â€” the 3 comes "out" via the chain rule

### ln(x) â€” The Natural Logarithm

$$\frac{d}{dx} \ln(x) = \frac{1}{x}$$

**Why ML loves it:**
- **Cross-entropy loss** = -Î£ yáµ¢ ln(Å·áµ¢) â€” the log turns products into sums
- **Log-likelihood** â€” taking the log of a likelihood function makes optimization easier because it converts products into sums (and products of small numbers become sums, which don't underflow)
- **Information theory** â€” entropy and KL divergence are defined with logarithms (you'll see this in Phase 3)

**With chain rule:** d/dx ln(g(x)) = g'(x)/g(x)

**Key identity that appears everywhere:**
$$\frac{d}{dx} \ln(f(x)) = \frac{f'(x)}{f(x)}$$

This is called the **logarithmic derivative**. It measures the *relative* rate of change â€” "what fraction of itself is f changing by?" This shows up constantly in maximum likelihood estimation (Lesson 21).

### sin(x) and cos(x) â€” The Oscillators

$$\frac{d}{dx} \sin(x) = \cos(x) \qquad \frac{d}{dx} \cos(x) = -\sin(x)$$

**Intuition:** sin and cos are 90Â° out of phase. When sin is at its peak (slope = 0), cos = 0. When sin is climbing fastest (at x = 0), cos = 1. The derivative of sin tracks cos exactly.

**Where they appear in ML:**
- **Positional encodings** in transformers use sin and cos to encode token positions
- **Fourier features** for encoding high-frequency patterns
- Less central than e^x and ln(x), but they show up in specific architectures

### â­ The Sigmoid Function â€” The Original Activation Function

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**What it does:** Squashes any real number into the range (0, 1). Large positive inputs â†’ close to 1. Large negative inputs â†’ close to 0. At x = 0, Ïƒ(0) = 0.5.

**Its beautiful derivative:**

$$\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))$$

The derivative of the sigmoid is expressed *in terms of itself*. This made it very convenient for early neural networks. The maximum derivative is 0.25 (at x = 0), which means gradients always shrink when passing through a sigmoid â€” this is one cause of the **vanishing gradient problem**.

**Derivation (good chain rule practice):**

Ïƒ(x) = (1 + e^(-x))â»Â¹

Using chain rule:
- Ïƒ'(x) = -1 Â· (1 + e^(-x))â»Â² Â· (-e^(-x))
- Ïƒ'(x) = e^(-x) / (1 + e^(-x))Â²
- Ïƒ'(x) = [1/(1 + e^(-x))] Â· [e^(-x)/(1 + e^(-x))]
- Ïƒ'(x) = Ïƒ(x) Â· [(1 + e^(-x) - 1)/(1 + e^(-x))]
- Ïƒ'(x) = Ïƒ(x) Â· [1 - 1/(1 + e^(-x))]
- **Ïƒ'(x) = Ïƒ(x) Â· (1 - Ïƒ(x))**

### tanh â€” The Centered Sigmoid

$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1$$

**Output range:** (-1, 1) â€” centered at zero, which is often better for optimization.

**Derivative:** tanh'(x) = 1 - tanhÂ²(x) â€” again expressible in terms of itself.

### ReLU â€” The Modern Default

$$\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}$$

**Derivative:**

$$\text{ReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x < 0 \\ \text{undefined} & \text{if } x = 0 \end{cases}$$

**Why ReLU won:** Its gradient is either 0 or 1 â€” never a fraction. This means gradients don't shrink when passing through ReLU layers, largely solving the vanishing gradient problem. In practice, we just define ReLU'(0) = 0 and it works fine.

**The tradeoff:** "Dead neurons" â€” if a neuron's input is always negative, its gradient is always 0, and it never learns. Variants like Leaky ReLU (small positive slope for x < 0) address this.

---

## Part 4: Integration â€” The Reverse Journey

### What Integration IS (Geometrically)

If differentiation asks "what's the slope?", integration asks "what's the area?"

**The definite integral** âˆ«â‚áµ‡ f(x) dx = the (signed) area between f(x) and the x-axis from x = a to x = b.

"Signed" means area below the x-axis counts as negative.

### The Fundamental Theorem of Calculus

**Differentiation and integration are inverses of each other.**

Part 1: If F(x) = âˆ«â‚Ë£ f(t) dt, then F'(x) = f(x).

(The derivative of the "area so far" function is the original function.)

Part 2: âˆ«â‚áµ‡ f(x) dx = F(b) - F(a), where F is any antiderivative of f.

(To compute a definite integral, find an antiderivative and evaluate at the endpoints.)

### Basic Antiderivatives (Reverse the Derivative Table)

| Function | Antiderivative |
|----------|---------------|
| xâ¿ | x^(n+1)/(n+1) + C (for n â‰  -1) |
| 1/x | ln\|x\| + C |
| eË£ | eË£ + C |
| sin(x) | -cos(x) + C |
| cos(x) | sin(x) + C |

The "+ C" is the **constant of integration** â€” a reminder that many functions have the same derivative (vertical shifts don't change slope).

### ğŸ”— ML & Alignment: Why Integration Matters

You won't integrate much by hand in ML, but you need to understand it conceptually because:

1. **Probability:** P(a â‰¤ X â‰¤ b) = âˆ«â‚áµ‡ p(x) dx â€” probability IS area under a density curve. Every time you say "the probability of X being between 1 and 3," you're computing an integral. This is the entire foundation of Phase 3.

2. **Expectation:** E[X] = âˆ« x Â· p(x) dx â€” the expected value IS a weighted integral. Loss functions in ML are often expectations.

3. **Normalization:** Probability densities must integrate to 1: âˆ« p(x) dx = 1. This constraint is why softmax exists â€” it ensures neural network outputs form a valid probability distribution.

4. **Evidence in Bayesian inference:** P(data) = âˆ« P(data|Î¸) P(Î¸) dÎ¸ â€” the denominator in Bayes' theorem is an integral over all possible parameter values. This integral is often intractable, which is why approximate methods (MCMC, variational inference) are so important.

---

## Part 5: Finding Maxima and Minima â€” This IS What Training Does

### Critical Points

A **critical point** is where f'(x) = 0 or f'(x) is undefined. These are candidates for maxima, minima, or saddle points.

**To find the minimum of a function:**
1. Take the derivative
2. Set it equal to zero: f'(x) = 0
3. Solve for x
4. Check whether it's a minimum (second derivative test)

**This is exactly what training a neural network does, in spirit.** Setting âˆ‚L/âˆ‚w = 0 for every weight w would give you the optimal weights. But in high dimensions with millions of parameters, you can't solve this system analytically, so gradient descent *iteratively approximates* the solution by taking steps toward where the gradient is zero.

### The Second Derivative Test

The **second derivative** f''(x) measures how the *slope itself* is changing â€” it's the curvature.

At a critical point where f'(x) = 0:
- f''(x) > 0 â†’ **local minimum** (concave up â€” bowl shape â€” the slope is increasing)
- f''(x) < 0 â†’ **local maximum** (concave down â€” hill shape â€” the slope is decreasing)
- f''(x) = 0 â†’ **inconclusive** (could be an inflection point)

**Example:**

f(x) = xÂ³ - 3x

f'(x) = 3xÂ² - 3 = 0 â†’ xÂ² = 1 â†’ x = Â±1

f''(x) = 6x

At x = 1: f''(1) = 6 > 0 â†’ **local minimum** at (1, -2)
At x = -1: f''(-1) = -6 < 0 â†’ **local maximum** at (-1, 2)

### ğŸ”— ML & Alignment: The Second Derivative Becomes the Hessian

In one dimension, f''(x) tells you curvature. In many dimensions, the matrix of all second partial derivatives â€” the **Hessian** â€” tells you curvature in every direction. You'll meet this properly in Lesson 14, but the intuition is the same: the Hessian tells you if you're at a bowl (minimum), a hill (maximum), or a saddle point. In neural networks, saddle points are far more common than local minima â€” this is a key insight you'll explore in Lesson 19.

---

## Part 6: Implicit Differentiation â€” Derivatives Without y = f(x)

### The Idea

So far, every function has been written as y = f(x) â€” y is *explicitly* defined in terms of x. But sometimes you have a relationship between x and y that isn't solved for y, like:

$$x^2 + y^2 = 25$$

This is a circle. It's not a function (it fails the vertical line test), but it still defines a *relationship* between x and y. At most points on the circle, if you zoom in, you see a smooth curve with a well-defined slope. How do you find that slope?

### The Technique

The trick: **differentiate both sides with respect to x**, treating y as a function of x (even though you can't write it explicitly). Whenever you differentiate a term containing y, tack on a dy/dx via the chain rule.

**Example:** xÂ² + yÂ² = 25

Differentiate both sides with respect to x:
- d/dx(xÂ²) = 2x
- d/dx(yÂ²) = 2y Â· (dy/dx)  â† chain rule! y is a function of x
- d/dx(25) = 0

So: 2x + 2y(dy/dx) = 0

Solve for dy/dx: **dy/dx = -x/y**

This makes geometric sense! At the point (3, 4) on the circle, the slope is -3/4. The tangent line tilts down to the right, which matches the circle curving away from the top.

**A more complex example:** xÂ³ + yÂ³ = 6xy

This is the *folium of Descartes* â€” a beautiful curve that loops through the origin.

Differentiate: 3xÂ² + 3yÂ²(dy/dx) = 6y + 6x(dy/dx)

Collect dy/dx terms: 3yÂ²(dy/dx) - 6x(dy/dx) = 6y - 3xÂ²

Factor: (3yÂ² - 6x)(dy/dx) = 6y - 3xÂ²

**dy/dx = (6y - 3xÂ²) / (3yÂ² - 6x) = (2y - xÂ²) / (yÂ² - 2x)**

### ğŸ”— ML Connection

Implicit differentiation is the conceptual ancestor of **automatic differentiation.** In a computation graph, you don't have an explicit formula for the loss as a simple function of one weight â€” it's defined *implicitly* through layers of computation. The chain rule applied through the graph (backprop) is doing the same essential thing: finding derivatives of quantities that are defined through relationships rather than explicit formulas.

It also appears directly in **constrained optimization** (Lesson 18): when you optimize subject to a constraint g(x, y) = 0, implicit differentiation tells you how y must change when x changes to stay on the constraint surface.

---

## Part 7: Integration by Substitution (u-Substitution) â€” The Chain Rule in Reverse

### The Idea

The chain rule says: d/dx f(g(x)) = f'(g(x)) Â· g'(x)

Reading this *backward* as an integration rule: if you see an integrand that looks like f'(g(x)) Â· g'(x), its antiderivative is f(g(x)).

The technique: let **u = g(x)**, so **du = g'(x) dx**. Replace everything in the integral with u's and du's, integrate in the simpler u-world, then substitute back.

### Step-by-Step Example

$$\int 2x \cos(x^2) \, dx$$

1. **Spot the inner function:** xÂ² looks like a good candidate for u
2. **Let u = xÂ²**, so **du = 2x dx**
3. The integral becomes: âˆ« cos(u) du
4. Integrate: sin(u) + C
5. Substitute back: **sin(xÂ²) + C**

Check by differentiating: d/dx sin(xÂ²) = cos(xÂ²) Â· 2x âœ“

### When the du Doesn't Match Perfectly

$$\int x \cdot e^{x^2} \, dx$$

Let u = xÂ². Then du = 2x dx, so x dx = du/2.

âˆ« x Â· e^(xÂ²) dx = âˆ« e^u Â· (du/2) = (1/2)e^u + C = **(1/2)e^(xÂ²) + C**

You often need to multiply/divide by constants to make the substitution work. You can always adjust constants, but you **cannot** adjust functions of x â€” if the "leftover" after substitution still contains x, you need a different approach.

### Definite Integrals: Change the Bounds

$$\int_0^1 2x(x^2 + 1)^3 \, dx$$

Let u = xÂ² + 1, du = 2x dx. When x = 0, u = 1. When x = 1, u = 2.

= âˆ«â‚Â² uÂ³ du = [uâ´/4]â‚Â² = 16/4 - 1/4 = **15/4**

### ğŸ”— ML Connection

Change of variables in integrals is essential for **probability.** When you transform a random variable â€” say, passing Gaussian noise through a neural network â€” you need u-substitution (or its multivariable cousin, the change-of-variables formula with a Jacobian determinant) to find the resulting distribution. This is the mathematical foundation of **normalizing flows,** a generative model architecture that transforms simple distributions into complex ones through invertible neural networks.

---

## Part 8: Integration by Parts â€” The Product Rule in Reverse

### The Formula

The product rule says: d/dx [u Â· v] = u'v + uv'

Rearranging: uv' = d/dx[uv] - u'v

Integrating both sides:

$$\int u \, dv = uv - \int v \, du$$

In words: pick one part of the integrand to differentiate (u) and the other to integrate (dv). The hope is that âˆ« v du is simpler than what you started with.

### The LIATE Rule for Choosing u

When deciding what to call u, prioritize in this order (the thing you **differentiate**):

**L**ogarithmic â†’ **I**nverse trig â†’ **A**lgebraic â†’ **T**rigonometric â†’ **E**xponential

The idea: logs and polynomials get *simpler* when differentiated, while exponentials and trig functions are easy to integrate. You want u to simplify under differentiation.

### Example 1: âˆ« x Â· eË£ dx

- Let u = x (algebraic â€” simplifies when differentiated), dv = eË£ dx
- Then du = dx, v = eË£
- âˆ« x eË£ dx = x eË£ - âˆ« eË£ dx = **x eË£ - eË£ + C = eË£(x - 1) + C**

### Example 2: âˆ« xÂ² sin(x) dx

This requires **two rounds** of integration by parts:

Round 1: u = xÂ², dv = sin(x) dx â†’ du = 2x dx, v = -cos(x)
= -xÂ² cos(x) + âˆ« 2x cos(x) dx

Round 2: u = 2x, dv = cos(x) dx â†’ du = 2 dx, v = sin(x)
= -xÂ² cos(x) + 2x sin(x) - âˆ« 2 sin(x) dx
= **-xÂ² cos(x) + 2x sin(x) + 2 cos(x) + C**

### Example 3: âˆ« ln(x) dx â€” A Trick

This one surprises people. Let u = ln(x), dv = dx:
- du = (1/x) dx, v = x
- âˆ« ln(x) dx = x ln(x) - âˆ« x Â· (1/x) dx = x ln(x) - x + C = **x(ln(x) - 1) + C**

### Example 4: The "Circular" Trick â€” âˆ« eË£ sin(x) dx

Sometimes integration by parts loops back to the original integral. When that happens, solve algebraically:

Let I = âˆ« eË£ sin(x) dx

Round 1: u = sin(x), dv = eË£ dx â†’ I = eË£ sin(x) - âˆ« eË£ cos(x) dx

Round 2: u = cos(x), dv = eË£ dx â†’ I = eË£ sin(x) - [eË£ cos(x) + âˆ« eË£ sin(x) dx]

I = eË£ sin(x) - eË£ cos(x) - I

2I = eË£(sin(x) - cos(x))

**I = eË£(sin(x) - cos(x)) / 2 + C**

### ğŸ”— ML Connection

Integration by parts appears when computing **expectations** of products (e.g., E[X Â· g(X)] for certain distributions), in deriving properties of the **Gaussian distribution,** and in the **ELBO (Evidence Lower Bound)** derivations used in variational autoencoders. It's also the foundation of **Stein's method** in probability, which has connections to score-based generative models.

---

## Part 9: Trigonometric Substitution â€” Handling Square Roots

### When to Use It

Trig substitution handles integrals involving these patterns:

| Expression | Substitution | Why it works |
|-----------|-------------|-------------|
| âˆš(aÂ² - xÂ²) | x = a sin(Î¸) | Because 1 - sinÂ²Î¸ = cosÂ²Î¸ eliminates the root |
| âˆš(aÂ² + xÂ²) | x = a tan(Î¸) | Because 1 + tanÂ²Î¸ = secÂ²Î¸ eliminates the root |
| âˆš(xÂ² - aÂ²) | x = a sec(Î¸) | Because secÂ²Î¸ - 1 = tanÂ²Î¸ eliminates the root |

### The Geometric Picture

For âˆš(aÂ² - xÂ²): draw a right triangle with hypotenuse a and one leg x. Then the other leg is âˆš(aÂ² - xÂ²). Setting x = a sin(Î¸) means Î¸ is the angle opposite the leg x. All the square root expressions become simple trig functions of Î¸.

### Example: âˆ« âˆš(4 - xÂ²) dx

This is the area under a semicircle of radius 2 â€” we should get something involving Ï€ if we do a definite integral.

Let x = 2 sin(Î¸), so dx = 2 cos(Î¸) dÎ¸

âˆš(4 - xÂ²) = âˆš(4 - 4sinÂ²Î¸) = 2âˆš(1 - sinÂ²Î¸) = 2 cos(Î¸)

âˆ« 2cos(Î¸) Â· 2cos(Î¸) dÎ¸ = 4 âˆ« cosÂ²(Î¸) dÎ¸

Use the identity cosÂ²(Î¸) = (1 + cos(2Î¸))/2:

= 4 Â· (Î¸/2 + sin(2Î¸)/4) + C = 2Î¸ + sin(2Î¸) + C

Since sin(2Î¸) = 2 sin(Î¸) cos(Î¸) = 2 Â· (x/2) Â· (âˆš(4-xÂ²)/2) = xâˆš(4-xÂ²)/2:

= **2 arcsin(x/2) + xâˆš(4-xÂ²)/2 + C**

### Example: âˆ« dx / (xÂ² + 9)

Let x = 3 tan(Î¸), dx = 3 secÂ²(Î¸) dÎ¸

xÂ² + 9 = 9 tanÂ²(Î¸) + 9 = 9 secÂ²(Î¸)

âˆ« 3 secÂ²(Î¸) / (9 secÂ²(Î¸)) dÎ¸ = (1/3) âˆ« dÎ¸ = Î¸/3 + C = **(1/3) arctan(x/3) + C**

### ğŸ”— ML Connection

Trig substitution is less common in day-to-day ML than the other integration techniques, but it appears in deriving the **normalizing constants** of certain probability distributions (especially the Student-t distribution, which involves (1 + xÂ²/Î½)^(-...) integrands) and in **kernel methods** where radial basis functions involve âˆš(||x - y||Â² + ÏƒÂ²) expressions.

---

## Part 10: Improper Integrals â€” Integrating to Infinity

### The Core Idea

What happens when the bounds of an integral go to Â±âˆ, or when the integrand blows up? These are **improper integrals,** and the key question is: **does the total area converge to a finite number, or does it diverge to infinity?**

### Type 1: Infinite Bounds

$$\int_1^{\infty} \frac{1}{x^2} \, dx = \lim_{b \to \infty} \int_1^b \frac{1}{x^2} \, dx = \lim_{b \to \infty} \left[-\frac{1}{x}\right]_1^b = \lim_{b \to \infty} \left(-\frac{1}{b} + 1\right) = 1$$

The area under 1/xÂ² from 1 to âˆ is exactly 1. It **converges.**

Compare with: âˆ«â‚^âˆ 1/x dx = lim_{bâ†’âˆ} [ln(x)]â‚áµ‡ = lim_{bâ†’âˆ} ln(b) = âˆ. This **diverges.**

### The p-Test: When Does 1/xáµ– Converge?

$$\int_1^{\infty} \frac{1}{x^p} \, dx \text{ converges if and only if } p > 1$$

This is worth memorizing. 1/x (p = 1) is the critical boundary case â€” it diverges (just barely). 1/xÂ² (p = 2) converges. 1/x^(1.001) converges. 1/x^(0.999) diverges. The boundary is sharp.

### Type 2: Integrand Blows Up

$$\int_0^1 \frac{1}{\sqrt{x}} \, dx = \lim_{\epsilon \to 0^+} \int_\epsilon^1 x^{-1/2} \, dx = \lim_{\epsilon \to 0^+} [2\sqrt{x}]_\epsilon^1 = 2 - 0 = 2$$

Even though 1/âˆšx â†’ âˆ as x â†’ 0, the area is finite. It **converges.**

### The Gaussian Integral â€” The Most Important Improper Integral in All of Statistics

$$\int_{-\infty}^{\infty} e^{-x^2} \, dx = \sqrt{\pi}$$

This result is remarkable â€” the area under the bell curve involves Ï€, even though there are no circles in sight. The proof uses a clever trick: square the integral, convert to polar coordinates, and the r integral becomes a simple u-substitution. You'll use this result constantly in Phase 3 when working with Gaussian (normal) distributions.

The full Gaussian PDF includes a normalizing constant: p(x) = (1/âˆš(2Ï€)) e^(-xÂ²/2). That 1/âˆš(2Ï€) is there precisely to make the integral equal 1, as a probability distribution requires.

### Comparison Test (Intuition)

If you can't evaluate an improper integral directly, compare it:
- If 0 â‰¤ f(x) â‰¤ g(x) and âˆ« g(x) converges â†’ âˆ« f(x) converges (it's smaller than something finite)
- If f(x) â‰¥ g(x) â‰¥ 0 and âˆ« g(x) diverges â†’ âˆ« f(x) diverges (it's bigger than something infinite)

### ğŸ”— ML & Alignment Connection

**Every probability distribution you'll ever use** requires improper integrals to verify it integrates to 1. The Gaussian, exponential, Cauchy, and Student-t distributions all integrate over infinite domains. Understanding convergence vs. divergence tells you which distributions have finite means, finite variances, or neither â€” the Cauchy distribution, for example, has no mean at all because the integral âˆ« x/(Ï€(1+xÂ²)) dx diverges. This matters for understanding **heavy-tailed phenomena** in ML, including the sometimes-surprising behavior of gradient norms during training.

---

## Part 11: Applied Optimization â€” Setting Up and Solving Word Problems

### The Framework

Applied optimization follows a consistent pattern:

1. **Draw a picture** and label everything
2. **Write the objective function** â€” what are you maximizing or minimizing?
3. **Write the constraint** â€” what relationship limits the variables?
4. **Use the constraint to eliminate a variable** â€” reduce to one variable
5. **Take the derivative, set it to zero, solve**
6. **Verify it's a max/min** (second derivative test or endpoint check)

### Example 1: The Classic Fence Problem

A farmer has 200 feet of fencing and wants to enclose the maximum rectangular area against a barn wall (one side doesn't need fencing).

1. Let x = length perpendicular to barn, y = length parallel to barn
2. **Objective:** Maximize A = xy
3. **Constraint:** 2x + y = 200 (two sides of length x, one of length y)
4. **Eliminate:** y = 200 - 2x, so A(x) = x(200 - 2x) = 200x - 2xÂ²
5. **Differentiate:** A'(x) = 200 - 4x = 0 â†’ x = 50, so y = 100
6. **Verify:** A''(x) = -4 < 0 â†’ maximum âœ“

**Maximum area = 5000 ftÂ²** with dimensions 50 Ã— 100.

### Example 2: Minimizing Distance

Find the point on the curve y = âˆšx closest to the point (3, 0).

1. **Objective:** Minimize DÂ² = (x - 3)Â² + (âˆšx)Â² = (x - 3)Â² + x (minimizing DÂ² avoids the square root, and the minimum occurs at the same point)
2. Let f(x) = xÂ² - 6x + 9 + x = xÂ² - 5x + 9
3. f'(x) = 2x - 5 = 0 â†’ x = 5/2
4. y = âˆš(5/2)
5. f''(x) = 2 > 0 â†’ minimum âœ“

### Example 3: Optimization with Trig

A window has the shape of a rectangle topped by a semicircle. If the perimeter is 12 feet, find the dimensions that maximize the area.

1. Let w = width (= diameter of semicircle), h = height of rectangular part
2. **Objective:** A = wh + Ï€(w/2)Â²/2 = wh + Ï€wÂ²/8
3. **Constraint:** Perimeter = w + 2h + Ï€(w/2) = w + 2h + Ï€w/2 = 12
4. **Eliminate:** h = (12 - w - Ï€w/2) / 2 = 6 - w/2 - Ï€w/4
5. Substitute into A, differentiate, set to zero, solve

The algebra gets messy, but the *pattern* is the same every time: objective, constraint, eliminate, differentiate, solve.

### ğŸ”— ML Connection

This "set up an objective function, then optimize" pattern is *literally* what ML does. In ML, the objective is the loss function, the constraints might be regularization or architecture choices, and the optimization is gradient descent. The skill of translating a real-world problem into "what am I minimizing and what are the constraints?" is the same skill you use when designing a loss function for a new task.

---

## Part 12: Series and Infinite Sums â€” When Adding Forever Gives a Finite Answer

### The Core Question

Can you add up infinitely many numbers and get a finite result? Sometimes yes, sometimes no. The study of **when** and **why** is the theory of series.

### Geometric Series â€” The Most Important Series in Practice

$$\sum_{n=0}^{\infty} r^n = \frac{1}{1 - r} \quad \text{when } |r| < 1$$

**Example:** 1 + 1/2 + 1/4 + 1/8 + ... = 1/(1 - 1/2) = 2

**Intuition:** Each term adds half of the remaining distance to 2. You never overshoot, but you get arbitrarily close.

**When |r| â‰¥ 1,** the series diverges â€” the terms don't shrink, so the sum grows without bound.

### The Harmonic Series â€” A Surprise

$$\sum_{n=1}^{\infty} \frac{1}{n} = 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \cdots = \infty$$

The terms go to zero, but the sum is infinite! The terms don't shrink *fast enough.* This is the series analog of âˆ«â‚^âˆ 1/x dx diverging.

**Key lesson:** Terms going to zero is **necessary** but **not sufficient** for convergence.

### p-Series

$$\sum_{n=1}^{\infty} \frac{1}{n^p} \text{ converges if and only if } p > 1$$

Same threshold as the improper integral, and for the same reason (the integral test connects them directly).

### Convergence Tests â€” A Quick Reference

| Test | When to use | How it works |
|------|------------|--------------|
| **Divergence test** | Always check first | If lim aâ‚™ â‰  0, the series diverges. (If = 0, inconclusive.) |
| **Geometric series** | aâ‚™ = arâ¿ | Converges iff \|r\| < 1, sum = a/(1-r) |
| **p-series** | aâ‚™ = 1/náµ– | Converges iff p > 1 |
| **Comparison** | Can bound aâ‚™ by a known series | 0 â‰¤ aâ‚™ â‰¤ bâ‚™ and Î£bâ‚™ converges â†’ Î£aâ‚™ converges |
| **Ratio test** | Terms with factorials or exponentials | lim \|aâ‚™â‚Šâ‚/aâ‚™\| < 1 â†’ converges; > 1 â†’ diverges |
| **Integral test** | aâ‚™ = f(n) with f positive, decreasing | Î£aâ‚™ and âˆ«f(x)dx converge/diverge together |
| **Alternating series** | Signs alternate: (-1)â¿ aâ‚™ | If aâ‚™ decreasing â†’ 0, the series converges |

### Power Series â€” Functions as Infinite Polynomials

A **power series** is an infinite polynomial: Î£ aâ‚™xâ¿. It converges for some values of x and diverges for others. The **radius of convergence R** defines the interval (-R, R) where the series converges.

Every Taylor series is a power series. The Taylor series of f(x) around x = 0 (also called the **Maclaurin series**) is:

$$f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} x^n$$

**Key examples:**

$$e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots \quad (R = \infty)$$

$$\sin(x) = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!} = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots \quad (R = \infty)$$

$$\frac{1}{1-x} = \sum_{n=0}^{\infty} x^n = 1 + x + x^2 + x^3 + \cdots \quad (R = 1)$$

$$\ln(1 + x) = \sum_{n=1}^{\infty} \frac{(-1)^{n+1} x^n}{n} = x - \frac{x^2}{2} + \frac{x^3}{3} - \cdots \quad (R = 1)$$

### ğŸ”— ML & Alignment Connection

**Geometric series** show up directly in **reinforcement learning:** the discounted return G = râ‚ + Î³râ‚‚ + Î³Â²râ‚ƒ + ... is a geometric-series-weighted sum of rewards. The discount factor Î³ < 1 ensures this converges, just like |r| < 1 ensures the geometric series converges.

**Taylor series** are the mathematical foundation of gradient descent. The first-order Taylor approximation f(x + Î´) â‰ˆ f(x) + f'(x)Î´ is exactly the "locally linear" model that gradient descent trusts for one step. The second-order approximation involves the Hessian and is the basis for Newton's method. You'll explore this in depth in Lesson 21.

**Power series** concepts also appear in **neural tangent kernel** theory, where the training dynamics of infinitely wide networks can be expressed as series expansions, and in **Singular Learning Theory,** where the "real log canonical threshold" involves analyzing singularities of loss functions â€” a topic you'll encounter toward the end of this curriculum.

---

## ğŸ“º Watch

### Primary (watch these in order)

- **3Blue1Brown â€” Essence of Calculus, full playlist (12 videos)**
  - https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr
  - **This is your main resource.** Grant builds calculus from geometric intuition exactly the way we built linear algebra. Watch the whole playlist before moving on.
  - Key chapters for ML:
    - Ch. 1: "The essence of calculus" â€” area under curves, the fundamental theorem
    - Ch. 2: "The paradox of the derivative" â€” limits and instantaneous change
    - Ch. 3: "Derivative formulas through geometry" â€” visual power rule, product rule
    - Ch. 6: "Implicit differentiation" â€” useful for understanding constrained optimization later
    - Ch. 7: "Limits" â€” the formal machinery (watch for completeness, don't obsess)
    - Ch. 10: "Taylor series" â€” critical for understanding why gradient descent works
    - Ch. 11: "Taylor series (geometric view)" â€” connects to the "zooming in" intuition

### Integration Techniques

- **Khan Academy â€” Integration techniques**
  - https://www.khanacademy.org/math/calculus-2
  - Use the u-substitution, integration by parts, and trig substitution sections
  - Also: improper integrals section, series and convergence tests sections

- **3Blue1Brown â€” "But what is a convolution?"**
  - https://www.youtube.com/watch?v=KuXjwB4LzSA
  - Not strictly integration technique, but connects integration to signal processing and neural networks

### Supplementary

- **Khan Academy â€” Calculus 1**
  - https://www.khanacademy.org/math/calculus-1
  - Use as supplementary practice if any rule feels shaky after 3B1B
  - Particularly recommended: Chain rule section, Optimization problems section

- **Professor Leonard â€” Calculus 2 playlist**
  - https://www.youtube.com/playlist?list=PLDesaqWTN6ESPaHy2QUKVaXNZuQNxkYQ_
  - Very thorough, lecture-style. Good for integration by parts, trig substitution, and series if you want someone to walk through many examples slowly

---

## ğŸ“– Read

- **MML Book, Chapter 5.1â€“5.3** â€” Differentiation of univariate functions, partial differentiation basics
  - https://mml-book.github.io/
  - Focus on: derivative rules, chain rule, and Taylor series (5.1â€“5.3 before moving to 5.4+ in Lesson 14)

- **Paul's Online Math Notes â€” Calculus I & II Review**
  - https://tutorial.math.lamar.edu/Classes/CalcI/CalcI.aspx
  - https://tutorial.math.lamar.edu/Classes/CalcII/CalcII.aspx
  - Excellent for quick lookup and practice problems. Especially useful sections:
    - Calculus I: Implicit differentiation, optimization
    - Calculus II: Integration by parts, trig substitution, improper integrals, series & convergence tests

---

## ğŸ”¨ Do

### Drill Set 1: Core Differentiation (Warm Up)

Differentiate each by hand. No calculator. Check by plugging into a derivative calculator afterward.

1. f(x) = 3xâ´ - 2xÂ² + 7x - 5
2. f(x) = (xÂ² + 1)(xÂ³ - x)  â† product rule
3. f(x) = (2x + 1)â·  â† chain rule
4. f(x) = sin(xÂ²)  â† chain rule
5. f(x) = e^(3xÂ² - x)  â† chain rule
6. f(x) = ln(xÂ² + 1)  â† chain rule
7. f(x) = x Â· e^x  â† product rule
8. f(x) = sin(x)/x  â† quotient rule or product rule with xâ»Â¹

### Drill Set 2: The Chain Rule (Critical for ML)

These are deliberately nested to build your chain-rule muscle:

1. f(x) = (sin(x))Â³
2. f(x) = e^(sin(x))
3. f(x) = ln(cos(x))
4. f(x) = Ïƒ(3x) where Ïƒ is the sigmoid â€” express your answer using Ïƒ(3x)
5. f(x) = (e^x + e^(-x))Â² â† this relates to coshÂ²(x)

### Drill Set 3: Finding Critical Points

For each function, find all critical points and classify them (min/max/inflection):

1. f(x) = xÂ³ - 6xÂ² + 9x + 1
2. f(x) = xe^(-x)  â† where does this peak?
3. f(x) = x - ln(x) for x > 0  â† this shape appears in KL divergence

### Drill Set 4: ML-Connected Exercises

1. **Sigmoid derivative:** Verify that Ïƒ'(x) = Ïƒ(x)(1 - Ïƒ(x)) by computing the derivative of 1/(1 + e^(-x)) from scratch using chain rule. Then plot Ïƒ(x) and Ïƒ'(x) in Python. Note where the derivative is largest and smallest.

2. **Cross-entropy gradient:** For L = -ln(Ïƒ(x)) (binary cross-entropy for the positive class), compute dL/dx. Simplify. What happens as Ïƒ(x) â†’ 1 (confident correct)? As Ïƒ(x) â†’ 0 (confident wrong)?

3. **Learning rate visualization:** Plot f(x) = xâ´ - 2xÂ² + 1 (a double-well potential). Implement gradient descent manually:
   - Start at x = -0.5
   - Try learning rates: 0.01, 0.1, 0.5, 1.0
   - Plot the trajectory for each. Which converges? Which oscillates? Which diverges?

4. **Taylor approximation:** Plot f(x) = e^x alongside its 1st, 2nd, and 3rd order Taylor approximations around x = 0. See how each term adds precision. This is the math behind "locally linear" â€” gradient descent uses the 1st order approximation.

### Drill Set 5: Implicit Differentiation

1. Find dy/dx for: xÂ² + xy + yÂ² = 7
2. Find the slope of the tangent line to xÂ³ + yÂ³ = 9xy at the point (2, 4)
3. Find dy/dx for: sin(xy) = x + y

### Drill Set 6: Integration Techniques

**U-substitution:**
1. âˆ« x Â· cos(xÂ²) dx
2. âˆ« e^(2x) / (1 + e^(2x)) dx
3. âˆ«â‚€Â¹ xÂ²(xÂ³ + 1)â´ dx

**Integration by parts:**
4. âˆ« x Â· ln(x) dx
5. âˆ« xÂ² Â· eË£ dx
6. âˆ« arctan(x) dx  â† (hint: let u = arctan(x), dv = dx)

**Trig substitution:**
7. âˆ« dx / âˆš(9 - xÂ²)
8. âˆ« âˆš(1 + xÂ²) dx  â† this is hard, don't worry if it takes a while

### Drill Set 7: Improper Integrals and Series

**Improper integrals:**
1. âˆ«â‚^âˆ 1/xÂ³ dx â€” evaluate and verify convergence
2. âˆ«â‚€^âˆ xe^(-x) dx â€” evaluate using integration by parts
3. Does âˆ«â‚^âˆ 1/(x ln(x)) dx converge or diverge? (hint: u-substitution)

**Series:**
4. Determine convergence/divergence: Î£ (1/3)â¿ for n = 0 to âˆ. If convergent, find the sum.
5. Determine convergence/divergence: Î£ n/(n+1) for n = 1 to âˆ
6. Use the ratio test on: Î£ n!/nâ¿ for n = 1 to âˆ
7. Write the first 4 nonzero terms of the Taylor series for f(x) = cos(x) around x = 0

### Drill Set 8: Applied Optimization

1. Find two positive numbers whose sum is 100 and whose product is maximized.
2. A box with a square base and open top must have a volume of 32,000 cmÂ³. Find the dimensions that minimize the amount of material used.
3. A 10-foot ladder leans against a wall. The base slides away at 1 ft/sec. How fast is the top sliding down when the base is 6 ft from the wall? â† (related rates â€” uses implicit differentiation)

---

## âœ… Self-Assessment: Am I Ready for Lesson 14?

Before moving on, you should be able to:

- [ ] Differentiate any polynomial, exponential, logarithmic, or trigonometric function
- [ ] Apply the chain rule confidently to nested functions (2â€“3 layers deep)
- [ ] Apply the product rule without hesitation
- [ ] Use implicit differentiation to find dy/dx
- [ ] Derive Ïƒ'(x) = Ïƒ(x)(1 - Ïƒ(x)) from scratch
- [ ] Find critical points and classify them using the second derivative test
- [ ] Set up and solve applied optimization problems (objective + constraint â†’ eliminate â†’ differentiate)
- [ ] Evaluate integrals using u-substitution
- [ ] Evaluate integrals using integration by parts (including choosing u via LIATE)
- [ ] Set up a trig substitution for integrals involving âˆš(aÂ² - xÂ²), âˆš(aÂ² + xÂ²), or âˆš(xÂ² - aÂ²)
- [ ] Evaluate improper integrals and determine convergence vs. divergence
- [ ] Determine whether a series converges using the appropriate test
- [ ] Write the Taylor/Maclaurin series for eË£, sin(x), cos(x), and 1/(1-x)
- [ ] Explain geometrically what a derivative, tangent line, and "locally linear" mean
- [ ] Explain why integration gives area under a curve
- [ ] Articulate why the chain rule is the mathematical foundation of backpropagation
- [ ] Explain why "zooming in" makes gradient descent work

---

## ğŸ“ Comprehensive Quiz: First-Semester Calculus / BC Calculus Refresher

> **Instructions:** Work these problems by hand, showing your reasoning. No calculator. This quiz covers the full scope of Lesson 13. Give yourself about 90 minutes.

---

### Question 1: Derivative Fundamentals + Chain Rule

Differentiate:

$$f(x) = \ln\!\Big(\frac{e^{3x}}{\sqrt{x^2 + 1}}\Big)$$

*Hint:* Simplify with log rules first, then differentiate. This tests whether you can combine logarithm properties with the chain rule â€” exactly the manipulation you'll need when working with log-likelihoods in ML.

---

### Question 2: Implicit Differentiation

The curve xÂ²y + xyÂ² = 6 passes through the point (1, 2).

**(a)** Find dy/dx using implicit differentiation.

**(b)** Find the equation of the tangent line at (1, 2).

---

### Question 3: The Sigmoid â€” An ML Rite of Passage

The sigmoid function is Ïƒ(x) = 1/(1 + e^(-x)).

**(a)** Derive Ïƒ'(x) from scratch using the chain rule. Show every step.

**(b)** The maximum value of Ïƒ'(x) is 1/4, occurring at x = 0. Explain in one sentence why this is a problem for deep neural networks. (This is the **vanishing gradient problem.**)

---

### Question 4: Applied Optimization

A rectangular poster must contain 150 inÂ² of printed area. The top and bottom margins are 3 inches each, and the side margins are 2 inches each. Find the overall dimensions of the poster that minimize the total amount of paper used.

---

### Question 5: Integration by Substitution

Evaluate:

$$\int_0^{\pi/2} \sin^3(x) \cos(x) \, dx$$

---

### Question 6: Integration by Parts

Evaluate:

$$\int x^2 \ln(x) \, dx$$

---

### Question 7: Trig Substitution

Evaluate:

$$\int \frac{dx}{x^2 \sqrt{x^2 + 4}}$$

*Hint:* Let x = 2 tan(Î¸).

---

### Question 8: Improper Integrals

Determine whether each integral converges or diverges. If it converges, find its value.

**(a)** $$\int_1^{\infty} \frac{x}{(x^2 + 1)^2} \, dx$$

**(b)** $$\int_0^{\infty} e^{-3x} \, dx$$

---

### Question 9: Series and Convergence

**(a)** Does the series converge or diverge? Find the sum if it converges.

$$\sum_{n=1}^{\infty} \frac{4}{3^n}$$

**(b)** Use the ratio test to determine convergence or divergence:

$$\sum_{n=0}^{\infty} \frac{n^2}{2^n}$$

**(c)** Write the Taylor series for e^(-xÂ²) around x = 0, up to the xâ¶ term. *(Hint: start from the known series for eË£ and substitute.)*

---

### Question 10: Connecting Calculus to ML â€” Conceptual

Answer each in 2â€“3 sentences:

**(a)** Gradient descent takes a step in the direction -Î±f'(x). Using the idea of Taylor series / local linearity, explain why a large learning rate Î± can cause the loss to *increase* instead of decrease.

**(b)** The ReLU activation function has derivative 1 for x > 0 and 0 for x < 0. The sigmoid's maximum derivative is 0.25. Explain why this difference matters when you apply the chain rule across 50 layers (i.e., multiply 50 derivative factors together).

**(c)** In reinforcement learning, the total discounted reward is G = râ‚ + Î³râ‚‚ + Î³Â²râ‚ƒ + ... where 0 < Î³ < 1. Why does this sum converge? What would happen if Î³ = 1?

---

### Quiz Answer Key

<details>
<summary>Click to reveal answers (try all problems first!)</summary>

**Q1:**
First simplify: ln(e^(3x) / âˆš(xÂ²+1)) = 3x - (1/2)ln(xÂ²+1)

f'(x) = 3 - (1/2) Â· 2x/(xÂ²+1) = 3 - x/(xÂ²+1) = **(3xÂ² + 3 - x) / (xÂ² + 1)**

**Q2:**
(a) Differentiate: 2xy + xÂ²(dy/dx) + yÂ² + 2xy(dy/dx) = 0

dy/dx(xÂ² + 2xy) = -(2xy + yÂ²)

**dy/dx = -(2xy + yÂ²) / (xÂ² + 2xy)**

At (1,2): dy/dx = -(4 + 4)/(1 + 4) = **-8/5**

(b) y - 2 = (-8/5)(x - 1) â†’ **y = (-8/5)x + 18/5**

**Q3:**
(a) Ïƒ(x) = (1 + e^(-x))^(-1)

Ïƒ'(x) = -1 Â· (1 + e^(-x))^(-2) Â· (-e^(-x)) = e^(-x) / (1 + e^(-x))Â²

= [1/(1+e^(-x))] Â· [e^(-x)/(1+e^(-x))]

= Ïƒ(x) Â· [(1 + e^(-x) - 1)/(1 + e^(-x))]

= **Ïƒ(x)(1 - Ïƒ(x))**

(b) When you chain 50 sigmoid layers via the chain rule, you multiply ~50 factors of Ïƒ'(x) â‰¤ 0.25. So the gradient shrinks by at least (0.25)^50 â‰ˆ 10^(-30) â€” effectively zero. The gradient signal vanishes and early layers can't learn.

**Q4:**
Let x = width of printed area, y = height of printed area. Then xy = 150, so y = 150/x.

Total paper: A = (x + 4)(y + 6) = (x + 4)(150/x + 6)

A = 150 + 6x + 600/x + 24 = 6x + 600/x + 174

A'(x) = 6 - 600/xÂ² = 0 â†’ xÂ² = 100 â†’ x = 10, y = 15

Total dimensions: **(10 + 4) Ã— (15 + 6) = 14 Ã— 21 inches**

A''(x) = 1200/xÂ³ > 0 â†’ minimum âœ“

**Q5:**
Let u = sin(x), du = cos(x) dx. When x = 0, u = 0; when x = Ï€/2, u = 1.

âˆ«â‚€Â¹ uÂ³ du = [uâ´/4]â‚€Â¹ = **1/4**

**Q6:**
Let u = ln(x), dv = xÂ² dx â†’ du = (1/x) dx, v = xÂ³/3

âˆ« xÂ² ln(x) dx = (xÂ³/3)ln(x) - âˆ« (xÂ³/3)(1/x) dx = (xÂ³/3)ln(x) - (1/3)âˆ« xÂ² dx

= **(xÂ³/3)ln(x) - xÂ³/9 + C = (xÂ³/9)(3ln(x) - 1) + C**

**Q7:**
Let x = 2tan(Î¸), dx = 2secÂ²(Î¸)dÎ¸, âˆš(xÂ²+4) = 2sec(Î¸)

âˆ« 2secÂ²(Î¸) / (4tanÂ²(Î¸) Â· 2sec(Î¸)) dÎ¸ = (1/4) âˆ« sec(Î¸)/tanÂ²(Î¸) dÎ¸ = (1/4) âˆ« cos(Î¸)/sinÂ²(Î¸) dÎ¸

Let u = sin(Î¸): = (1/4) Â· (-1/sin(Î¸)) + C = -1/(4sin(Î¸)) + C

Since x = 2tan(Î¸), we have sin(Î¸) = x/âˆš(xÂ²+4):

= **-âˆš(xÂ²+4) / (4x) + C**

**Q8:**
(a) u = xÂ² + 1: âˆ«â‚‚^âˆ (1/2)uâ»Â² du = [-1/(2u)]â‚‚^âˆ = 0 + 1/4 = **1/4, converges**

(b) âˆ«â‚€^âˆ e^(-3x) dx = [-e^(-3x)/3]â‚€^âˆ = 0 + 1/3 = **1/3, converges**

**Q9:**
(a) Geometric with a = 4/3, r = 1/3: Sum = (4/3)/(1 - 1/3) = (4/3)/(2/3) = **2, converges**

(b) aâ‚™ = nÂ²/2â¿. Ratio: (n+1)Â²Â·2â¿ / (nÂ²Â·2^(n+1)) = ((n+1)/n)Â² Â· (1/2) â†’ **1/2 < 1, converges**

(c) e^u = 1 + u + uÂ²/2! + uÂ³/3! + ... Substitute u = -xÂ²:

**e^(-xÂ²) = 1 - xÂ² + xâ´/2 - xâ¶/6 + ...**

**Q10:**
(a) Gradient descent trusts the first-order Taylor approximation: L(w - Î±L') â‰ˆ L(w) - Î±(L')Â². This approximation is only accurate near w. If Î± is too large, you step beyond where the linear approximation is valid â€” the actual curved surface may go uphill even though the tangent plane pointed downhill.

(b) With ReLU, the chain rule product across 50 layers is 1^50 = 1 (for active neurons) â€” the gradient passes through unchanged. With sigmoid, the product is at most (0.25)^50 â‰ˆ 10^(-30) â€” effectively zero. Sigmoid's fractional derivatives multiply to nothing; ReLU's derivative of 1 preserves the signal.

(c) It's a geometric series with ratio Î³, so it converges to at most r_max/(1-Î³). If Î³ = 1, the series becomes râ‚ + râ‚‚ + râ‚ƒ + ..., which diverges if rewards are consistently positive â€” the agent would value the infinite future equally with the present, making optimization impossible.

</details>

---

## ğŸ”— Looking Ahead

This lesson gives you the single-variable foundations. Here's how each subsequent lesson builds on them:

| Lesson | How It Extends This Material |
|--------|------------------------------|
| **14: Matrix Calculus** | Derivatives of functions with *vector/matrix* inputs â†’ gradients, Jacobians |
| **15: Gradients** | The gradient generalizes "slope" to multiple dimensions â€” points uphill |
| **16: Chain Rule** | The single-variable chain rule extends to computation graphs â€” this IS backprop |
| **17: Optimization** | Gradient descent is "follow the negative derivative" scaled up to millions of parameters |
| **18: Constrained Opt.** | What if you need the minimum *subject to a constraint*? Lagrange multipliers |
| **19: Loss Landscapes** | The geometry of high-dimensional critical points â€” why saddle points dominate |

Every concept in Lessons 14â€“19 is a multivariable generalization of something in this lesson. If this lesson is solid, the rest of Phase 2 will feel like natural extensions rather than new territory.


