# Lesson 20: Multiple Integration and Change of Variables â€” Jacobians Meet Probability

[â† Loss Landscapes](lesson-19-loss-landscapes.md) | [Back to TOC](../README.md) | [Next: Taylor Expansions â†’](lesson-21-taylor-expansions.md)

---

> **Why this lesson exists:** Every time you compute an expectation, normalize a probability distribution, or transform random variables, you're doing a multivariable integral. The Jacobian determinant â€” which you already know as a volume-scaling factor from Lesson 7 â€” reappears as the essential correction term when you change variables in an integral. This is the mathematical engine behind normalizing flows, reparameterization tricks in VAEs, and the reason certain distributions are conjugate. Without this, the Bayesian computation in Phase 3 will feel like magic; with it, it's just calculus.

## ğŸ¯ Core Concepts

### Double and Triple Integrals â€” Geometry First

- **A double integral** âˆ¬_R f(x,y) dA computes the "volume under the surface" f over region R. Think of it as stacking infinitely many single integrals. The iterated integral âˆ«âˆ« f(x,y) dx dy works from inside out.
- **Fubini's theorem:** you can switch the order of integration (dx dy â†” dy dx) when f is "nice enough" (continuous). This is surprisingly powerful â€” sometimes one order is easy and the other is nightmarish.
- **For ML:** computing E[f(X,Y)] = âˆ¬ f(x,y) p(x,y) dx dy is a double integral over the joint density. Marginalizing out Y means âˆ« p(x,y) dy â€” a single integral that "projects" the joint onto X.

### Change of Variables â€” The Jacobian Returns

- **The key formula:** if you substitute u = g(x,y), v = h(x,y), the integral transforms as:
  âˆ¬ f(x,y) dx dy = âˆ¬ f(gâ»Â¹(u,v)) |det(Jâ»Â¹)| du dv
  where J is the Jacobian matrix of the transformation (âˆ‚(u,v)/âˆ‚(x,y)).
- **Polar coordinates** are the classic example: x = r cos Î¸, y = r sin Î¸, and the Jacobian determinant is r. That's why the area element becomes r dr dÎ¸.
- **This IS the Lesson 7 determinant** in action: the Jacobian determinant measures how the transformation stretches or compresses volume locally. At each point, it's the factor by which infinitesimal areas change.
- **For probability:** if X has density p_X(x) and Y = g(X), then p_Y(y) = p_X(gâ»Â¹(y)) |det(dgâ»Â¹/dy)|. This is how you derive the density of a transformed random variable. Normalizing flows use this formula at every layer.

### The Gaussian Integral and Its Children

- **The foundational integral:** âˆ«_{-âˆ}^{âˆ} e^{-xÂ²} dx = âˆšÏ€. You can't do this with single-variable techniques â€” the proof uses polar coordinates (change of variables!) on the 2D version âˆ¬ e^{-(xÂ²+yÂ²)} dx dy = Ï€.
- **Every Gaussian calculation** descends from this: the normalization constant of the normal distribution, multivariate Gaussian integrals, moment-generating functions, partition functions. If you understand this one integral, you understand half of probability theory.
- **Multivariate Gaussian:** for p(x) = N(Î¼, Î£), the normalization involves âˆ« exp(-Â½(x-Î¼)áµ€Î£â»Â¹(x-Î¼)) dx = (2Ï€)^{d/2} |det(Î£)|^{1/2}. The determinant of the covariance matrix appears because of the change-of-variables Jacobian when diagonalizing Î£ using its eigenvectors.

### Monte Carlo Integration â€” When Integrals Can't Be Solved

- **Most integrals in ML are intractable.** Bayesian posteriors, partition functions, marginal likelihoods â€” these involve integrals over high-dimensional spaces with no closed form.
- **Monte Carlo idea:** approximate âˆ« f(x)p(x) dx by drawing samples xâ‚,...,xâ‚™ from p and computing (1/n) Î£ f(xáµ¢). This converges by the law of large numbers, regardless of dimension.
- **Importance sampling:** if you can't sample from p directly, sample from a proposal distribution q and reweight: âˆ« f(x)p(x)dx = âˆ« f(x)(p(x)/q(x))q(x)dx â‰ˆ (1/n) Î£ f(xáµ¢)p(xáµ¢)/q(xáµ¢). This is the foundation of particle filters and some MCMC methods.

## ğŸ“º Watch â€” Primary

1. **3Blue1Brown â€” "The Gaussian integral" (if available)**
   - The polar coordinates proof is a classic visualization opportunity
2. **Khan Academy â€” Multivariable calculus: double integrals, change of variables**
   - Solid worked examples with geometric intuition

## ğŸ“– Read â€” Primary

- **"Mathematics for Machine Learning" (MML) by Deisenroth** â€” Chapter 6.5â€“6.7 (Change of variables, integrals)
- **Paul's Online Math Notes â€” Multiple Integrals**

## ğŸ”¨ Do

- Compute âˆ¬ xÂ²y dx dy over the unit disk using polar coordinates. Feel the Jacobian factor r appear.
- Derive the PDF of Y = XÂ² when X ~ N(0,1) using the change-of-variables formula. Verify it's a chi-squared distribution with 1 df.
- Implement Monte Carlo estimation of Ï€ by sampling random points in [0,1]Â² and counting those inside the unit circle. Plot convergence vs sample size.
- Compute the normalization constant of a 2D Gaussian with covariance Î£ = [[2,1],[1,3]] by diagonalizing Î£ and using the change-of-variables formula.

## ğŸ”— ML Connection

- **Normalizing flows** are literally a chain of change-of-variables transformations, each with a tractable Jacobian determinant. The whole architecture is designed to make this integral formula efficient.
- **The reparameterization trick** in VAEs: instead of sampling z ~ q(z|x), write z = Î¼ + ÏƒÎµ where Îµ ~ N(0,1). This is a change of variables that makes the gradient flow through the sampling step.
- **Bayesian computation** is fundamentally about high-dimensional integrals. Every technique in Lesson 36 is a strategy for approximating intractable integrals.

## ğŸ§  Alignment Connection

Bayesian reasoning â€” which requires integration over parameter spaces â€” is central to alignment. Questions like "how confident should we be that this model is safe?" require integrating over possible model behaviors weighted by their probability. The **evidence integral** P(safe behavior) = âˆ« P(safe | Î¸) P(Î¸ | data) dÎ¸ is exactly the kind of high-dimensional integral you study here. Tractable approximations to these integrals (variational inference, MCMC) are the computational backbone of uncertainty quantification in AI safety.
