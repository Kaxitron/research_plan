# Lesson 27: Partial Differential Equations â€” Diffusion, Heat, and Generative Models

[â† Neural ODEs](lesson-26-neural-odes.md) | [Back to TOC](../README.md) | [Next: Probability Distributions â†’](../phase3-prob-stats/lesson-28-probability.md)

---

> **Why this lesson exists:** Diffusion models â€” the engine behind DALL-E, Stable Diffusion, and modern image generation â€” are built on stochastic PDEs. The forward process adds noise according to a heat equation; the reverse process denoises using a learned score function. You also need PDEs for the Fokker-Planck equation (the probability distribution of SGD), the Black-Scholes equation (decision theory under uncertainty), and the deep connections between probability, physics, and optimization that run through alignment theory.

## ğŸ¯ Core Concepts

### What's a PDE?

- **An ODE** has one independent variable (time). A **partial differential equation** has multiple: the heat equation u_t = ku_{xx} involves both time t and space x. The unknown u(x,t) is a function of multiple variables.
- **We only need a few key PDEs,** not the full PDE theory. The three essential ones for ML: the heat equation, the Fokker-Planck equation, and their stochastic versions.
- **The geometric picture:** where an ODE describes flow along curves, a PDE describes how entire fields (temperature distributions, probability densities, image features) evolve.

### The Heat Equation â€” Diffusion as Smoothing

- **u_t = kâˆ‡Â²u.** The temperature at each point changes at a rate proportional to how much it differs from its neighbors (the Laplacian âˆ‡Â²u). Hot spots cool; cold spots warm. Everything smooths toward the average.
- **The solution is a convolution with a Gaussian:** given initial temperature u(x,0), the solution at time t is u(x,t) = u(x,0) * G_t(x) where G_t is a Gaussian with variance 2kt. Diffusion IS Gaussian blurring, where the blur width grows with âˆšt.
- **This is the forward process of diffusion models.** Start with a data sample (the "initial temperature"). Add Gaussian noise progressively. After enough time, everything is just noise â€” the "equilibrium temperature" is uniform randomness.

### The Fokker-Planck Equation â€” How Probability Evolves

- **For the SDE** dX = f(X)dt + Ïƒ dB_t (deterministic drift f plus noise Ïƒ), the probability density p(x,t) evolves according to: âˆ‚p/âˆ‚t = -âˆ‚(fp)/âˆ‚x + (ÏƒÂ²/2)âˆ‚Â²p/âˆ‚xÂ². This IS a heat equation with an extra drift term.
- **For SGD:** the Fokker-Planck equation describes the probability distribution over weight space during stochastic training. The steady-state solution gives the Boltzmann distribution p(W) âˆ exp(-L(W)/T) mentioned in Lesson 26. This is the rigorous version of "SGD explores the loss landscape like a particle in a heat bath."
- **For alignment:** the Fokker-Planck equation tells you the probability that training reaches different solutions. If you could solve it (usually you can't), you'd know the probability of arriving at an aligned vs misaligned model.

### Score Functions and Denoising

- **The score function** âˆ‡_x log p(x) is the gradient of log-probability. It points toward regions of high probability â€” "uphill" on the probability landscape.
- **Score matching:** you can learn the score function from data without knowing the normalization constant of p. This bypasses the intractable integral problem that plagues other generative models.
- **Diffusion model reversal:** the forward SDE adds noise: dX = f(X)dt + Ïƒ dB_t. The REVERSE SDE that undoes this requires the score: dX = [f(X) - ÏƒÂ²âˆ‡_x log p(X,t)]dt + Ïƒ dBÌ„_t. Train a neural network to approximate the score, and you can generate samples by running the reverse process from pure noise.
- **The connection to ODEs:** there's also a deterministic ODE (the "probability flow ODE") that generates the same distribution as the reverse SDE but without noise. Neural ODEs meet diffusion models.

### Separation of Variables and Fourier Series

- **The standard technique** for solving linear PDEs: assume the solution factors as u(x,t) = X(x)T(t). This converts one PDE into two ODEs, which you can solve separately.
- **Fourier series:** the general solution to the heat equation is a sum of sinusoidal modes, each decaying exponentially at a rate determined by its frequency. High-frequency modes (sharp features) decay fastest â€” this is why diffusion blurs fine details first and coarse structure last.
- **For ML:** this frequency-dependent decay explains why diffusion models generate images from coarse to fine â€” the reverse process reconstructs low frequencies (overall layout) before high frequencies (texture and detail).

## ğŸ“º Watch â€” Primary

1. **3Blue1Brown â€” "But what is a partial differential equation?"**
   - https://www.youtube.com/watch?v=ly4S0oi3Yz8
   - *Covers the heat equation with beautiful visualizations of Fourier modes.*
2. **Welch Labs â€” "But How Do AI Images and Videos Actually Work?" (Ch. 9, w/ 3B1B)**
   - https://www.youtube.com/watch?v=iv-5mZ_9CPY
   - *ğŸ“– Book: Welch Labs, "The Illustrated Guide to AI," Ch. 9: Video & Image Generation*
   - *Diffusion models, CLIP, and the math of turning text into images. The perfect application of everything in this lesson â€” the reverse diffusion process IS a PDE solved backward in time.*
3. **Sander Dieleman â€” "Diffusion models explained"**
   - *The best accessible explanation connecting PDEs to diffusion generative models.*

## ğŸ“º Watch â€” Secondary

3. **Steve Brunton â€” "Fourier Series and Heat Equation"**
   - https://www.youtube.com/c/Eigensteve
4. **3Blue1Brown â€” "Solving the heat equation" (Diff Eq Ch. 3)**
   - https://www.youtube.com/watch?v=ToIXSwZ1pJU
   - *How Fourier series actually solve the heat equation â€” watch right after the primary 3B1B PDE video.*
5. **3Blue1Brown â€” "But what is a Fourier series?" (Diff Eq Ch. 4)**
   - https://www.youtube.com/watch?v=r6sGWTCMz2k
   - *The visual intuition for Fourier decomposition. Essential for understanding how diffusion models work.*

## ğŸ“º Watch â€” Optional

6. **3Blue1Brown â€” "e^(iÏ€) in 3.14 minutes" (Diff Eq Ch. 5)**
   - https://www.youtube.com/watch?v=v0YEaeIClKY
   - *A beautiful 3-minute aside on Euler's identity. Not directly needed but deepens appreciation for complex exponentials in Fourier analysis.*

## ğŸ“– Read â€” Primary

- **"Understanding Diffusion Models: A Unified Perspective" by Calvin Luo**
  - https://arxiv.org/abs/2208.11970
  - *Comprehensive tutorial connecting all the pieces: SDEs, score matching, Fokker-Planck.*

## ğŸ”¨ Do

- Simulate the 1D heat equation numerically: start with a step function, watch it smooth into a Gaussian. Implement both explicit Euler and spectral (Fourier) methods.
- Implement a toy 2D diffusion model: forward process (add noise to MNIST digits), learn the score function with a small neural net, reverse process (denoise). See images emerge from noise.
- Solve the Fokker-Planck equation for the double-well potential L(x) = (xÂ²-1)Â². Show that the steady-state distribution concentrates at the two minima with relative probability determined by the "temperature" ÏƒÂ².
- Visualize how Fourier modes decay: decompose an image into Fourier components, multiply each by exp(-kÂ²t) for increasing t. Watch the image blur progressively â€” this is diffusion.

## ğŸ”— ML & Alignment Connection

- **Diffusion models** are the most successful generative models in 2024-2026. They're pure applied PDE theory.
- **The Fokker-Planck equation** is the theoretical foundation for understanding SGD's implicit bias and the distribution of trained models.
- **Score-based methods** bypass the normalization problem that plagues energy-based models and Boltzmann machines.

- **Controllable generation:** if diffusion models generate content by following a PDE, can we modify the PDE to guarantee the output satisfies safety constraints? Classifier-free guidance is a step in this direction.
- **The Fokker-Planck perspective on training** could, in principle, give probabilistic guarantees about where training ends up. This is a theoretical alignment tool.
---

## ğŸ“ Time to Take the Exam

You've completed Phase 2. From single-variable derivatives to PDEs governing diffusion models, you now speak the language of change and dynamics.

ğŸ‘‰ **[Exam 2B: Differential Equations â€” The Language of Dynamics](../assessments/exam-2b-differential-equations.md)**
