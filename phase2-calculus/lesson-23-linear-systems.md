# Lesson 23: Linear Systems and Phase Portraits â€” Eigenvalues Determine Everything

[â† Intro to ODEs](lesson-22-intro-odes.md) | [Back to TOC](../README.md) | [Next: Gradient Flow â†’](lesson-24-gradient-flow.md)

---

> **Why this lesson exists:** Linear ODEs (dx/dt = Ax) are the one case where we can solve everything completely â€” and the answer is entirely determined by the eigenvalues of A. Since the behavior near any fixed point of a nonlinear system is governed by its linearization (the Jacobian), eigenvalue analysis is the universal tool for understanding local dynamics. You already know eigenvalues from Lesson 8. Now you'll see what they *do* â€” they control whether systems converge, diverge, oscillate, or spiral. This is the mathematical foundation for understanding training stability.

## ğŸ¯ Core Concepts

### Linear Systems: dx/dt = Ax

- **The simplest 2D system:** dx/dt = ax + by, dy/dt = cx + dy, or in matrix form: d**x**/dt = A**x** where A is a 2Ã—2 matrix. The entire behavior of this system is determined by the eigenvalues of A.
- **Why linear systems matter:** even for nonlinear systems (like gradient flow on a complicated loss), the behavior *near a fixed point* is determined by the linearization â€” the Jacobian matrix evaluated at that point. So understanding linear systems gives you local behavior everywhere.
- **The solution:** x(t) = e^{At}x(0), where e^{At} is the matrix exponential. You don't need to compute this directly. What matters is: if A has eigenvalues Î»â‚, Î»â‚‚ with eigenvectors vâ‚, vâ‚‚, then the solution is x(t) = câ‚e^{Î»â‚t}vâ‚ + câ‚‚e^{Î»â‚‚t}vâ‚‚. Each eigendirection evolves independently, at a rate determined by its eigenvalue.

### The Eigenvalue Classification â€” A Complete Map

The eigenvalues of A determine the **phase portrait** â€” the complete qualitative picture of all trajectories:

**Real eigenvalues, both negative (Î»â‚ < Î»â‚‚ < 0):** **Stable node.** All trajectories converge to the origin. They approach along the slow eigendirection (the one with the smaller |Î»|). This is what a well-conditioned loss minimum looks like â€” gradient descent converges from all directions.

**Real eigenvalues, both positive (0 < Î»â‚ < Î»â‚‚):** **Unstable node.** All trajectories flee the origin. This is a local maximum of the loss â€” training always escapes.

**Real eigenvalues, opposite signs (Î»â‚ < 0 < Î»â‚‚):** **Saddle point.** Trajectories approach along the stable eigendirection (Î»â‚ < 0) and flee along the unstable eigendirection (Î»â‚‚ > 0). Almost all trajectories eventually escape. Saddle points are the dominant critical points in high-dimensional loss landscapes.

**Complex eigenvalues (Î» = Î± Â± Î²i):**
- Î± < 0: **Stable spiral.** Trajectories spiral inward to the origin. This happens when the loss landscape has rotational character near a minimum â€” the condition number and the Hessian's off-diagonal structure create oscillation.
- Î± > 0: **Unstable spiral.** Trajectories spiral outward.
- Î± = 0: **Center.** Trajectories are closed orbits â€” they cycle forever without converging or diverging. This is the boundary between stable and unstable.

### Condition Number Revisited

- **You learned in Lesson 11** that the condition number Îº = |Î»_max|/|Î»_min| measures how "elongated" the transformation is. Now you see this dynamically: for a stable node with eigenvalues -1 and -100, the system converges 100x faster along one direction than another. Trajectories slam into the slow direction and then crawl along it.
- **For gradient descent on L = wâ‚Â²/2 + 50wâ‚‚Â²/2:** the Hessian eigenvalues are 1 and 100 (Îº = 100). Gradient flow converges along wâ‚‚ in ~0.01 time units but along wâ‚ in ~1 time unit. The zigzag you see in gradient descent plots is exactly the mismatched eigenvalue scales causing the trajectory to bounce off the steep walls of the elongated bowl.
- **Adam and preconditioning** effectively equalize the eigenvalues, turning an elongated bowl into a round one. In phase portrait terms: they transform a badly-conditioned stable node into a well-conditioned one.

### The Jacobian and Linearization

- **For nonlinear systems dx/dt = f(x):** the behavior near a fixed point x* is determined by the **Jacobian matrix** J = âˆ‚f/âˆ‚x evaluated at x*. The Jacobian is the matrix of all partial derivatives â€” it IS the linear approximation to f near x*.
- **The Hartman-Grobman theorem:** if all eigenvalues of J have nonzero real parts (no purely imaginary eigenvalues), then the nonlinear system looks qualitatively identical to the linear system dx/dt = Jx near the fixed point. The phase portrait of the linearization IS the local phase portrait of the full nonlinear system.
- **For gradient flow on a loss L:** the Jacobian of -âˆ‡L is the negative Hessian -H. So the eigenvalues of the Hessian at a critical point determine the local training dynamics. Positive Hessian eigenvalues â†’ negative eigenvalues of the flow â†’ stable (this is a minimum). Mixed Hessian eigenvalues â†’ saddle. This connects your Lesson 11 knowledge directly to dynamics.

### Phase Portraits as Complete Pictures

- **A phase portrait** shows representative trajectories overlaid on the vector field. It gives you the complete qualitative story: where does the system go from any initial condition?
- **Basins of attraction:** in a nonlinear system with multiple stable fixed points, the phase portrait reveals which initial conditions flow to which equilibrium. The boundaries between basins are called **separatrices** â€” they often pass through saddle points.
- **For neural networks:** different random initializations place you in different basins. The structure of basins in weight space determines the distribution of trained models. Understanding phase portraits is understanding this distribution.

## ğŸ“º Watch â€” Primary

1. **3Blue1Brown â€” "Differential equations, studying the unsolvable" (continued)**
   - Re-watch the section on phase portraits and eigenvalue classification if you haven't internalized it
2. **Steve Brunton â€” "Phase Portraits for 2D Systems" and "Stability and Eigenvalues" (Dynamical Systems playlist, videos 3â€“5)**
   - https://www.youtube.com/c/Eigensteve
   - *Brunton draws phase portraits by hand and connects eigenvalues to every type. Excellent visual treatment.*

## ğŸ“º Watch â€” Secondary

3. **MIT OCW â€” Strogatz "Nonlinear Dynamics and Chaos" (Lectures 5â€“6)**
   - Strogatz's lectures on 2D linear systems and phase plane classification are the gold standard
4. **Maththebeautiful â€” "Systems of ODEs and Phase Portraits"**
   - More worked examples if you want additional practice

## ğŸ“– Read â€” Primary

- **"Nonlinear Dynamics and Chaos" by Steven Strogatz** â€” Chapter 5 (Linear Systems) and Chapter 6.1â€“6.3 (Phase Plane)
  - *Chapter 5 is the definitive geometric treatment of the eigenvalue classification. Every case is illustrated with beautiful phase portraits. Chapter 6 extends to nonlinear systems via linearization.*

## ğŸ“– Read â€” Secondary

- **Steve Brunton & Nathan Kutz â€” "Data-Driven Science and Engineering"** â€” Chapter 7.1â€“7.3
  - https://www.databookuw.com/
  - *Applied treatment connecting dynamical systems to data, including computational examples*

## ğŸ”¨ Do

- **Build the eigenvalue classification:** for each 2Ã—2 matrix below, compute eigenvalues, predict the phase portrait type, then simulate and plot with Python to verify:
  - A = [[-1, 0], [0, -3]] (stable node, well-separated eigenvalues)
  - A = [[-1, 0], [0, -1]] (stable star node, repeated eigenvalue)
  - A = [[1, 0], [0, -2]] (saddle point)
  - A = [[-1, 2], [-2, -1]] (stable spiral â€” complex eigenvalues!)
  - A = [[0, 1], [-1, 0]] (center â€” pure imaginary eigenvalues)
- **Condition number visualization:** plot gradient flow trajectories for L(wâ‚, wâ‚‚) = wâ‚Â²/2 + Îºwâ‚‚Â²/2 with Îº = 1, 10, 100. Watch the zigzagging get worse as Îº increases. Then implement a "preconditioned" version that divides each gradient component by the corresponding Hessian eigenvalue â€” see the zigzag vanish.
- **Linearization practice:** for the nonlinear system dx/dt = x - xÂ³, dy/dt = -y, find all fixed points. At each one, compute the Jacobian and classify the fixed point. Draw the full phase portrait.
- **Key exercise:** the loss function L(wâ‚, wâ‚‚) = (wâ‚Â² - 1)Â² + wâ‚‚Â² has three critical points: (0,0), (1,0), (-1,0). Compute the Hessian at each. From the eigenvalues, determine which are minima, maxima, or saddle points. Predict the gradient flow behavior near each. Then simulate gradient descent from 20 random initial conditions and overlay on a contour plot to verify.

## ğŸ”— ML & Alignment Connection

- **Hessian eigenvalues â†’ training behavior:** this is the most direct application. The spectrum of the Hessian at a critical point tells you everything about local convergence: the largest eigenvalue limits your learning rate, the smallest determines how slow the slowest direction converges, and the condition number is their ratio.
- **Saddle points dominate in high dimensions:** in a d-dimensional loss landscape, a random critical point has roughly d/2 positive and d/2 negative Hessian eigenvalues â€” it's a saddle. True local minima become exponentially rare as d increases. This is why gradient descent works despite non-convexity: it just escapes saddles along the unstable directions.
- **Preconditioning and natural gradient:** Adam, RMSProp, and natural gradient methods all modify the dynamics to improve conditioning. They effectively change the phase portrait near critical points from elongated to round.

- **Basin structure as alignment landscape:** if aligned behavior corresponds to certain basins and misaligned behavior to others, the relative sizes and positions of basins determine alignment probability. Phase portraits visualize this directly.
- **Saddle escape determines capabilities:** when training encounters a saddle point, *which* unstable direction it escapes along determines what capability is learned. The choice of optimizer and noise level influences this selection â€” a lever for alignment.
- **Sensitivity to initialization:** if two basins are nearby and the separatrix between them is complex, small changes to initialization could flip between aligned and misaligned outcomes. Understanding this sensitivity is critical for safety analysis.