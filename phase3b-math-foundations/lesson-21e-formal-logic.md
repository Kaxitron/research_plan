# Lesson 21e: Formal Logic â€” Self-Reference, Limits, and AI Reasoning

[â† Topology](lesson-21d-topology.md) | [Back to TOC](../README.md) | [Next: Single Neuron â†’](../phase4-neural-networks/lesson-22-single-neuron.md)

---

> **Why this lesson exists:** MIRI's agent foundations research â€” arguably the most theoretically ambitious alignment program â€” is built on formal logic. Logical uncertainty (referenced in Lesson 30), LÃ¶b's theorem (why agents can't reason about their own trustworthiness in certain ways), and GÃ¶del's incompleteness theorems (fundamental limits on self-knowledge) all require formal logic vocabulary. When alignment researchers worry about "an AI system reasoning about its own reasoning," the obstacles they identify are logical in nature. This lesson gives you enough logic to read those arguments and understand what they're proving.

## ðŸŽ¯ Core Concepts

### Propositional and Predicate Logic

- **Propositional logic** deals with statements that are true or false, connected by AND (âˆ§), OR (âˆ¨), NOT (Â¬), IMPLIES (â†’), and IF AND ONLY IF (â†”).
  - A **tautology** is true regardless of the truth values of its components: P âˆ¨ Â¬P is always true.
  - **Modus ponens:** from P and P â†’ Q, conclude Q. The fundamental rule of inference.
  - **Contrapositive:** P â†’ Q is equivalent to Â¬Q â†’ Â¬P. Essential for reasoning about AI: "if the AI is safe, it passes the test" is equivalent to "if it fails the test, it's not safe."

- **Predicate logic (first-order logic)** adds quantifiers:
  - âˆ€x (for all x): "for every possible input, the model is safe"
  - âˆƒx (there exists x): "there exists an input that makes the model unsafe"
  - The negation of âˆ€x P(x) is âˆƒx Â¬P(x). To disprove "the model is always safe," you need just ONE counterexample.

- **Why this matters for alignment:** safety properties are logical statements. "The model never produces harmful output" is âˆ€x Â¬Harmful(Output(model, x)). Proving this is a universal statement â€” you need it to hold for ALL inputs. Finding a violation is an existential statement â€” you need just ONE bad input. This asymmetry is fundamental to why safety is harder than capability.

### Formal Systems and Provability

- **A formal system** consists of axioms (starting truths), rules of inference (how to derive new truths), and theorems (everything derivable). First-order arithmetic (Peano Arithmetic, or PA) is the standard formal system for reasoning about numbers.
- **Provability vs. truth:** in a formal system, a statement is *provable* if it can be derived from the axioms. It's *true* (in the intended model) if it holds in the standard natural numbers. These are NOT the same â€” the gap between them is GÃ¶del's discovery.
- **Consistency:** a system is consistent if it never proves both P and Â¬P. Inconsistent systems are useless (they prove everything). We hope our formal systems are consistent, but proving consistency is itself subject to fundamental limits.

### GÃ¶del's Incompleteness Theorems

- **First incompleteness theorem:** any consistent formal system strong enough to describe basic arithmetic contains statements that are true but unprovable within the system. There are mathematical truths that the system can state but never prove or disprove.
  - **The GÃ¶del sentence G says, roughly: "this statement is not provable in this system."** If G is provable, the system proves something false (G says it's unprovable). If G is unprovable, it's true (it correctly says it's unprovable). So G is true but unprovable.
  - This is a diagonal argument again â€” the same self-referential trick as the halting problem (Lesson 21b). GÃ¶del essentially built a "program" (formal statement) that talks about its own provability.

- **Second incompleteness theorem:** a consistent formal system cannot prove its own consistency. If a system proves "I am consistent," then it is actually inconsistent.
  - **Alignment implication:** an AI system using formal reasoning cannot prove, within its own reasoning system, that its reasoning system is consistent. There will always be a gap between what the AI can verify about itself and what is actually true about itself.

### LÃ¶b's Theorem â€” The Obstacle to Self-Trust

- **LÃ¶b's theorem:** in any formal system containing basic arithmetic, if the system can prove "if I can prove P, then P is true," then the system can actually prove P.
  - Formally: if PA âŠ¢ (Prov(âŒˆPâŒ‰) â†’ P), then PA âŠ¢ P.
  - In English: a formal system cannot assert "everything I prove is true" unless it can prove literally everything (making it inconsistent or trivial).

- **Why this matters for AI:** suppose you want an AI system that reasons: "If my reasoning process concludes that action X is safe, then X is actually safe." LÃ¶b's theorem says this kind of self-trust is impossible within a consistent formal system â€” unless the conclusion is already independently provable.

- **The AI self-trust problem:** we want AI systems that can verify their own reasoning. But LÃ¶b's theorem creates obstacles: the AI cannot, within its own logic, establish that its own proofs are reliable in general. This is one reason MIRI researchers believe alignment requires going beyond standard formal reasoning frameworks.

### Self-Reference and Fixed Points

- **Self-reference** is the common thread: GÃ¶del sentences refer to their own provability. The halting problem involves programs analyzing themselves. LÃ¶b's theorem concerns systems asserting their own reliability.
- **The diagonal lemma (GÃ¶del's fixed point theorem):** for any property P, there exists a sentence that says "I have property P." This is the formal mechanism that makes self-reference possible and creates the incompleteness phenomena.
- **Quining:** in programming, a quine is a program that outputs its own source code. This is self-reference made computational. The connection to GÃ¶del: a GÃ¶del sentence is essentially a logical quine â€” a formula that "quotes" itself.
- **Relevance to embedded agency:** an AI system embedded in the world it's reasoning about must deal with self-reference constantly. It's a physical system reasoning about physical systems (including itself). The formal logic tells us this creates inescapable limitations.

### Logical Uncertainty â€” Reasoning About What You Can't Compute

- **Logical uncertainty** (referenced in Lesson 30) is uncertainty about the consequences of your own deductive system. You know the axioms, you know the rules, but you haven't computed all the theorems yet. What probability should you assign to an unproven mathematical conjecture?
- **Why standard probability theory doesn't cover this:** Bayesian probability handles *empirical* uncertainty (uncertainty about facts). Logical uncertainty is different â€” the facts are determined by the axioms, but you don't know them yet because computing them takes too long.
- **MIRI's "Logical Induction" paper** (Garrabrant et al., 2016) proposes a framework for logical uncertainty that satisfies desirable rationality properties. It uses a market metaphor: beliefs are priced like assets, and a "logical inductor" is a belief system that can't be exploited by any computationally bounded trader.
- **Connection to AI alignment:** a sufficiently powerful AI faces massive logical uncertainty. It can't compute all consequences of its training objective, it can't fully simulate its own future behavior, and it can't verify all properties of its own code. How it handles this uncertainty affects whether it behaves safely.

## ðŸ“º Watch â€” Primary

1. **Veritasium â€” "Math Has a Fatal Flaw" (GÃ¶del's Incompleteness)**
   - https://www.youtube.com/watch?v=HeQX2HjkcNo
   - *Excellent accessible treatment of incompleteness with visual explanations*
2. **Numberphile â€” "GÃ¶del's Incompleteness Theorem"**
   - Multiple videos available â€” Prof. Marcus du Sautoy's version is good
3. **Robert Miles â€” "Embedded Agency"** (if available)
   - Connects logical limitations to AI alignment

## ðŸ“º Watch â€” Secondary

4. **Computerphile â€” "GÃ¶del's Incompleteness Theorem"**
   - More technical treatment with computer science flavor
5. **PBS Infinite Series â€” "LÃ¶b's Theorem and GÃ¶del"**
   - Connects LÃ¶b's theorem to game theory and cooperation

## ðŸ“– Read â€” Primary

- **"GÃ¶del, Escher, Bach" by Douglas Hofstadter** â€” Chapters 1-9
  - The classic treatment of self-reference, formal systems, and incompleteness. Dense but deeply rewarding. The dialogues between chapters build intuition before the formal chapters deliver the proofs.
- **"GÃ¶del's Proof" by Ernest Nagel and James Newman**
  - Short (100 pages), clear, self-contained explanation of the first incompleteness theorem. If GEB is too long, read this instead.

## ðŸ“– Read â€” Secondary

- **MIRI's "Logical Induction" by Garrabrant et al. (2016)**
  - https://arxiv.org/abs/1609.03543
  - The technical paper on logical uncertainty. Read the introduction and Section 1 for the motivation; the full technical details are very dense.
- **"Embedded Agency" by Demski & Garrabrant (MIRI)**
  - https://arxiv.org/abs/1902.09469
  - Section on "reasoning about reasoning" uses the logic from this lesson directly.
- **LessWrong â€” "LÃ¶b's Theorem" posts**
  - https://www.lesswrong.com/tag/lob-s-theorem
  - The rationalist community's treatment with alignment applications

## ðŸ“– Read â€” Going Deep

- **"Computability and Logic" by Boolos, Burgess, and Jeffrey**
  - The standard textbook for the mathematical logic underlying computability and incompleteness
- **"The Logic of Provability" by George Boolos**
  - Advanced treatment of provability logic, LÃ¶b's theorem, and self-referential reasoning

## ðŸ”¨ Do

- **GÃ¶del sentence construction:** write out (informally) a GÃ¶del sentence for a simple formal system. Walk through the diagonal argument step by step. Convince yourself it's true but unprovable.
- **Propositional logic warmup:** prove that (P â†’ Q) is equivalent to (Â¬Q â†’ Â¬P) using a truth table. Then prove that Â¬(âˆ€x P(x)) is equivalent to âˆƒx Â¬P(x). Apply these to alignment: rephrase "if the model is aligned, it passes the evaluation" and its contrapositive.
- **Quine exercise:** write a Python quine (a program that prints its own source code) without reading the source file. This is self-reference made concrete. Appreciate how the trick works â€” it's the same fixed-point mechanism as GÃ¶del's construction.
- **LÃ¶b's theorem thought exercise:** suppose an AI uses a formal proof system and reasons: "I've proven that action X is safe, and everything I prove is true, so X is safe." Explain, using LÃ¶b's theorem, why this chain of reasoning is problematic. What would the AI need to do differently?
- **Key exercise:** an AI alignment researcher proposes: "we'll make the AI prove, within its own formal system, that it is aligned." Using GÃ¶del's second incompleteness theorem, explain the fundamental obstacle. Then discuss: what *can* the AI prove about itself, and what must be verified externally?

## ðŸ”— ML Connection

- **Neural networks as proof search:** some recent work frames neural network computation as a form of bounded proof search in a logical system. The network "looks for" arguments that the output should be a particular token. This perspective connects deep learning to logic.
- **Chain-of-thought reasoning** in LLMs is a form of explicit logical derivation. Understanding what formal logic can and can't do helps evaluate the reliability of chain-of-thought.
- **Formal verification of neural networks** (proving that outputs stay within bounds for all inputs in a domain) uses the logic from this lesson. The limitations (Rice's theorem from 21b) and the possibilities (verification of specific properties) are both logical.

## ðŸ§  Alignment Connection

- **Embedded agency** (one of MIRI's core research areas) is about agents reasoning about themselves while being part of the world they reason about. GÃ¶del's incompleteness theorems and LÃ¶b's theorem are the formal obstacles that make this hard.
- **Logical uncertainty** is how AI systems should handle the gap between what they can prove and what is true. Getting this wrong can lead to overconfidence (the AI acts on unverified beliefs) or paralysis (the AI refuses to act because it can't prove safety). MIRI's logical induction is one proposed solution.
- **The AI self-improvement problem:** if an AI system modifies its own code, can it verify that the modification preserves safety? GÃ¶del's second theorem suggests fundamental limits: the system can't prove its own consistency, let alone prove that a modified version remains consistent. This is a formal argument for external oversight.
- **Cooperation between AI systems:** LÃ¶b's theorem has been connected to cooperation in the Prisoner's Dilemma. If two AI systems can read each other's source code, they can achieve mutual cooperation by reasoning about their mutual reasoning â€” but only with careful handling of the self-referential logic.
