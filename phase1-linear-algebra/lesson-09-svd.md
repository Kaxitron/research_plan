# Lesson 9: Singular Value Decomposition (SVD)

[â† Eigenvalues](lesson-08-eigenvalues.md) | [Back to TOC](../README.md) | [Next: Dot Products and Projections â†’](lesson-10-dot-products.md)

---

## ğŸ¯ Core Concepts

- **SVD: every matrix can be decomposed as A = UÎ£Váµ€**
  - V: rotate (align input with "natural axes")
  - Î£: scale (stretch/shrink â€” these are the singular values)
  - U: rotate again (align output with "natural axes" of output space)
- **SVD works for ANY matrix** â€” rectangular, rank-deficient, any shape. Unlike eigendecomposition, SVD is universal.
- **Singular values Ïƒâ‚ â‰¥ Ïƒâ‚‚ â‰¥ ... â‰¥ 0** are always non-negative and ordered
- **Rank = number of non-zero singular values**
- **Low-rank approximation:** keep only top-k singular values â†’ best rank-k approximation (Eckartâ€“Young theorem)
- **Connection to eigendecomposition:** singular values of A are square roots of eigenvalues of Aáµ€A

## ğŸ“º Watch â€” Primary

1. **Steve Brunton â€” "SVD: Overview"**
   - https://www.youtube.com/watch?v=nbBvuuNVfco
   - Brunton's SVD series (5-7 videos) is outstanding: intuition, computation, applications.
2. **Steve Brunton â€” "SVD: Matrix Approximation"**
   - https://www.youtube.com/watch?v=xy3QyzhiuY4

## ğŸ“º Watch â€” Secondary

3. **Visual Kernel â€” "SVD Visualized"**
   - https://www.youtube.com/watch?v=vSczTbgc8Rc
4. **MIT OCW â€” Strang, Lecture 29: "Singular Value Decomposition"**
   - https://www.youtube.com/watch?v=TX_vooSnhm8
5. **Maththebeautiful â€” "SVD Part 1"**

## ğŸ“– Read â€” Primary

- **MML Book, Chapter 4.5** (SVD) â€” https://mml-book.github.io/
- **"We Recommend a Singular Value Decomposition" by Kalman (2002)** â€” freely available PDF

## ğŸ“– Read â€” Secondary

- **Interactive Linear Algebra (GT), Chapter 6**
- **Gregory Gundersen â€” "SVD as Simply as Possible"**
  - https://gregorygundersen.com/blog/2018/12/10/svd/

## ğŸ”¨ Do

- Implement SVD on a small matrix by hand (compute Aáµ€A, find eigenvalues/eigenvectors for V, then get U and Î£)
- **Image compression project:** Load a grayscale image as a matrix. Compute SVD. Reconstruct with k = 1, 5, 10, 50, 100 singular values. Watch detail emerge.
- **Compute compression ratio:** original = mÃ—n numbers. Rank-k SVD = mÃ—k + k + kÃ—n numbers. When does this save space?

## ğŸ”— ML Connection

SVD is *everywhere* in ML and alignment research:

- **Attention heads are low-rank:** W_Q and W_K project through a rank bottleneck. SVD reveals what information survives.
- **LoRA (Low-Rank Adaptation):** Fine-tuning via low-rank updates Î”W = BA. This IS the SVD insight.
- **Probing in interpretability:** Finding directions that predict labels is SVD-adjacent.
- **"Mathematical Framework for Transformer Circuits"** uses SVD-like decompositions extensively.

### SVD and PCA: The Same Coin

PCA asks you to find the eigenvectors of the covariance matrix C = (1/n)Xáµ€X. You could eigendecompose Xáµ€X directly â€” but notice what happens if you take the SVD of the data matrix X = UÎ£Váµ€ instead:

Xáµ€X = (UÎ£Váµ€)áµ€(UÎ£Váµ€) = VÎ£áµ€Uáµ€UÎ£Váµ€ = VÎ£Â²Váµ€

Since Uáµ€U = I (U is orthogonal), the U drops out entirely. What remains is the eigendecomposition of Xáµ€X â€” with V as the eigenvectors and Î£Â² as the eigenvalues. So **the right singular vectors (V) from SVD of your data ARE the principal components**, and the squared singular values are proportional to the variance along each direction. You don't need to form Xáµ€X at all â€” SVD on X gives you PCA for free, and is more numerically stable because it avoids squaring the singular values.
