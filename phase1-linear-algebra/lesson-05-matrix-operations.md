# Lesson 5: Matrix Operations Deep Dive

[â† Transformations](lesson-04-transformations.md) | [Back to TOC](../README.md) | [Next: Rank and Null Space â†’](lesson-06-rank-nullspace.md)

---

## ğŸ¯ Core Concepts

- **Matrix-vector multiplication: two equivalent views**
  - Row view: each output entry is a dot product (pattern matching)
  - Column view: the output is a linear combination of the columns
  - Both are true simultaneously â€” being able to switch between them is a superpower
- **Matrix-matrix multiplication:** composition of transformations
  - AB â‰  BA in general (rotating then scaling â‰  scaling then rotating)
  - But (AB)C = A(BC) â€” associativity
- **Transpose:** reflection over the diagonal; rows become columns
  - (AB)áµ€ = Báµ€Aáµ€ â€” the "reverse order" rule

## ğŸ“º Watch â€” Primary

1. **3Blue1Brown Ch. 3â€“4** (review, focusing on the two views of multiplication)
2. **MIT OCW â€” Strang, Lecture 2 & 3**
   - Strang's four ways to see matrix multiplication is uniquely powerful
   - https://www.youtube.com/watch?v=QVKj3LADCnA

## ğŸ“º Watch â€” Secondary

3. **Khan Academy â€” "Matrix multiplication"** series
4. **Professor Leonard â€” "Matrix Multiplication"**
   - https://www.youtube.com/c/ProfessorLeonard

## ğŸ“– Read

- **MML Book, Chapter 2.2** (matrices and matrix operations)
- **"Introduction to Linear Algebra" by Strang** â€” Chapter 1 on matrix multiplication is the gold standard

## ğŸ”¨ Do

- Implement matrix multiplication from scratch in Python (no NumPy) â€” three nested loops
- Then implement it BOTH ways: (a) row-dot-product view, (b) linear-combination-of-columns view
- Verify they give the same answer
- **Speed test:** compare your implementation to NumPy's â€” GPUs are matrix multiplication accelerators

## ğŸ”— ML Connection

The forward pass through a neural network IS a sequence of matrix multiplications. When interpretability researchers decompose attention into QK and OV circuits, they're factoring matrix products. Understanding that Wâ‚’ Â· Wáµ¥ can be treated as a single matrix (the OV circuit) â€” that's composition of transformations. The "Mathematical Framework for Transformer Circuits" paper leans heavily on this.
