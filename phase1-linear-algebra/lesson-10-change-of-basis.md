# Lesson 10: Change of Basis, Norms, and Special Matrices

[â† Dot Products](lesson-09-dot-products.md) | [Back to TOC](../README.md) | [Next: Linear Algebra Capstone â†’](lesson-11-capstone.md)

---

> **Why this lesson exists:** When interpretability researchers say "let's look at this in the feature basis instead of the neuron basis," they mean a change of basis. When training uses weight decay, gradient clipping, or layer normalization, those involve norms. When analyzing loss landscapes, you need positive definite matrices. This lesson collects the "connective tissue" concepts that glue everything together.

## ðŸŽ¯ Core Concepts

### Change of Basis

- **The same vector has different coordinates in different bases.** A vector doesn't change â€” its description does. Just like "5 feet" and "1.524 meters" describe the same height in different units.
- **If B is a matrix whose columns are the new basis vectors**, then Bâ»Â¹v converts v's coordinates from the standard basis to the new basis. B converts back.
- **A transformation in a new basis:** If A is a transformation in the standard basis, then Bâ»Â¹AB is the *same transformation* described in the basis B. The matrix looks different, but it does the same thing to space.
- **Eigendecomposition AS change of basis:** A = PDPâ»Â¹ means "convert to the eigenbasis (Pâ»Â¹), scale each eigenvector direction (D), convert back (P)." The matrix looks *diagonal* â€” trivially simple! â€” in its eigenbasis.
- **SVD AS two changes of basis:** A = UÎ£Váµ€ means "convert input to V-basis, scale by Î£, convert output to U-basis." Every matrix is just scaling in the right coordinate systems.

### Norms â€” Measuring Size

- **L2 norm (Euclidean):** ||v||â‚‚ = âˆš(vâ‚Â² + vâ‚‚Â² + ... + vâ‚™Â²) â€” straight-line distance from origin. The "default" norm.
- **L1 norm (Manhattan):** ||v||â‚ = |vâ‚| + |vâ‚‚| + ... + |vâ‚™| â€” taxicab distance. Encourages sparsity (many components = 0).
- **Lâˆž norm (max):** ||v||âˆž = max(|vâ‚|, ..., |vâ‚™|) â€” largest component. Used in adversarial robustness.
- **Frobenius norm (for matrices):** ||A||_F = âˆš(sum of all squared entries) â€” treats the matrix as one long vector.
- **Spectral norm:** ||A||â‚‚ = largest singular value Ïƒâ‚ â€” the maximum stretching factor of the transformation.
- **Norm balls:** the set of all vectors with ||v|| â‰¤ 1. L2 ball = circle. L1 ball = diamond. Lâˆž ball = square. The *shape* of the norm ball determines what "closeness" means.

### Special Matrices

- **Symmetric matrices** (A = Aáµ€):
  - Eigenvalues are always real (never complex)
  - Eigenvectors are always orthogonal
  - Can always be diagonalized: A = QDQáµ€ where Q is orthogonal
  - **Where they appear:** covariance matrices, Hessians, kernel matrices, attention score matrices (before masking)
  
- **Positive definite matrices:** all eigenvalues > 0
  - The quadratic form xáµ€Ax > 0 for all nonzero x â€” geometrically, this is a "bowl" shape
  - Hessian being positive definite at a point = local minimum
  - **Where they appear:** loss landscape curvature, covariance matrices, Fisher information matrix

- **Positive semi-definite (PSD):** all eigenvalues â‰¥ 0
  - xáµ€Ax â‰¥ 0 for all x â€” bowl or flat, never a saddle
  - Covariance matrices are always PSD
  - Gram matrices (Aáµ€A) are always PSD

- **Trace:** tr(A) = sum of diagonal entries = sum of eigenvalues
  - tr(AB) = tr(BA) â€” cyclic property
  - ||A||Â²_F = tr(Aáµ€A) â€” Frobenius norm via trace
  - Shows up constantly in ML loss functions and gradient computations

## ðŸ“º Watch â€” Primary

1. **3Blue1Brown â€” "Change of basis" (Ch. 13)**
   - https://www.youtube.com/watch?v=P2LTAUO1TdA
   - *The visual of the same transformation looking complicated in one basis and simple in another is THE key insight. This is WHY we diagonalize.*

## ðŸ“º Watch â€” Secondary

2. **3Blue1Brown â€” "Abstract vector spaces" (Ch. 15)**
   - https://www.youtube.com/watch?v=TgKwz5Ikpc8
   - Functions as vectors, polynomials as vectors. The abstraction that lets you apply linear algebra to activation spaces and function spaces.
3. **MIT OCW â€” Strang, Lecture 25: "Symmetric Matrices and Positive Definiteness"**
   - https://www.youtube.com/watch?v=UCc9q_cAhho
4. **MIT OCW â€” Strang, Lecture 5: "Transposes, Permutations, Spaces"**
   - https://www.youtube.com/watch?v=JibVXBElKL0
5. **Steve Brunton â€” "Matrix Norms"**
   - Applied treatment of norms with data science context

## ðŸ“– Read â€” Primary

- **MML Book, Chapter 3.1â€“3.3** (norms, inner products, distances)
- **MML Book, Chapter 4.1** (determinant and trace)
- **MML Book, Chapter 2.7.2** (basis change)

## ðŸ“– Read â€” Secondary

- **Interactive Linear Algebra (GT), Chapter 5.3** â€” change of basis with interactive visualizations
- **Stanford CS229 Math Review** â€” https://cs229.stanford.edu/section/cs229-linalg.pdf
  - Concise summary of LA concepts most used in ML: norms, positive definiteness, matrix calculus

## ðŸ”¨ Do

- **Change of basis exercise:** Take A = [[2, 1], [1, 2]]. Compute eigenvectors. Construct P. Verify Pâ»Â¹AP = D is diagonal. Reconstruct A from PDPâ»Â¹. See the same transformation from two perspectives.
- **Norm comparison:** Generate 1000 random vectors in â„Â¹â°. Compute L1, L2, Lâˆž norms. Plot histograms. Notice L1 â‰¥ L2 â‰¥ Lâˆž always â€” why?
- **Norm ball visualization:** Plot the unit ball for L1, L2, and Lâˆž norms in 2D. See the diamond, circle, and square. Understand why L1 encourages sparsity (the diamond's corners are on the axes â€” sparse solutions!).
- **Positive definiteness:** Create a symmetric matrix. Compute eigenvalues. If all positive, verify xáµ€Ax > 0 for random x. If some negative, find x where xáµ€Ax < 0.
- **Key exercise:** Take the covariance matrix of 2D data. Show it's symmetric. Show it's PSD. Find eigenvalues and eigenvectors â€” these are the PCA directions and variances.

## ðŸ”— ML Connection

**Change of basis** is fundamental to interpretability. When researchers say "analyze this in the feature basis instead of the neuron basis," they mean a change of basis. Sparse autoencoders find a new basis (feature directions) where activations look sparse and interpretable, rather than dense and opaque in the neuron basis.

**Norms** show up everywhere in training:
- **Weight decay** = L2 regularization = adding Î»||W||Â² to the loss = penalizing large weights
- **L1 regularization** encourages sparsity â€” many weights become exactly zero
- **Gradient clipping** = if ||âˆ‡L|| > threshold, scale it down. Prevents exploding gradients.
- **Layer normalization** = dividing activations by their norm. Used in every transformer layer.
- **Spectral norm** of weight matrices bounds how much a layer amplifies signals

**Positive definiteness** of the Hessian tells you about loss landscape curvature. The ratio of largest to smallest Hessian eigenvalue (the **condition number**) measures how "elongated" the landscape is â€” high condition numbers make optimization slow (steep in one direction, flat in another).

## ðŸ§  Alignment Connection

**Singular Learning Theory** (SLT) cares deeply about the Hessian and its spectral properties at singularities in the loss landscape. Phase transitions â€” when a model suddenly "learns" a new capability â€” correspond to changes in effective dimensionality, measured through eigenvalue analysis. Understanding norms and positive definiteness gives you the vocabulary for this cutting-edge alignment theory.

The **neuron basis vs. feature basis** distinction is arguably the central insight of modern interpretability. Individual neurons don't correspond to clean concepts (they're "polysemantic"). But in a different basis â€” the feature basis found by sparse autoencoders â€” activations decompose into interpretable features. This is literally a change of basis making the representation interpretable.
